# [GPT-3: Language Models are Few-Shot Learners (NIPS 2020)](https://drive.google.com/file/d/1vMkMurPFQYrGednPe8So80hufMKdw92E/view?usp=drivesdk)
## 动机
增强大型预训练语言模型在小数据量不可知任务上的性能
scaling up language models greatly improves task-agnostic,
few-shot performance

## 方法
- an autoregressive language model (Transformer) with 175 billion
parameters 自回归模型指当前输出y(t)完全基于历史输出y(t-i)，而非x
- 

## References
- [ChatGPT: The End of Online Exam Integrity? (Arxiv)](https://arxiv.org/abs/2212.09292)
- [OpenAI](https://openai.com/blog/chatgpt/)