---
title: Large Language Model
author: yangtx
date: 2023-05-05 21:56:00 +0100
categories: [Machine Learning, Natural Language Processing]
tags: [natural-language-processing]     # TAG names should always be lowercase
math: true
---

## Large Language Model

### [LLaMA 2023](https://drive.google.com/file/d/19Wxke_OIBftyL5dQrpm-azz4Ugz-x1ge/view?usp=drivesdk)
Open and Efficient Foundation Language Models
基本上是对 GPT3+PaLM+GPTNeo的综合

- Architecture
  - **Pre-normalization** $\rightarrow$ improve the
  training stability: normalize (RMSNorm) the input of each transformer sub-layer
  - **SwiGLU activation function** $\rightarrow$ improve the performance
  - **Rotary Embeddings** $\rightarrow$ replace the absolute positional embeddings

- Training
  - only use publicly available data
  - Pre-training data
    - CommonCrawl
    - C4
    - Github
    - Wikipedia
    - Books
    - ArXiv
    - StackExchange
  - Tokenize method
    - bytepair encoding (BPE) algorithm
    - contains roughly 1.4T tokens after tokenization

- References
  - [publication](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)
  - [GitHub Model Card](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)

### [GPT-4 (arXiv 2023)](https://drive.google.com/file/d/1MFIW_ph8zELfbBD6ndgTOlzkpw8DoGV-/view?usp=drivesdk)
GPT-4 Technical Report

- Reference
  - [arXiv](https://arxiv.org/pdf/2303.08774.pdf)

