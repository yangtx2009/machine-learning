<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.2.0/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@6.7.0"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.2.7"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.2.0/dist/index.umd.min.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, data) => {
        const {
          Markmap
        } = getMarkmap();
        window.mm = Markmap.create('svg#mindmap', getOptions == null ? void 0 : getOptions(), data);
      })(() => window.markmap,null,{"t":"heading","d":1,"p":{"lines":[0,1]},"v":"Machine Learning","c":[{"t":"heading","d":2,"p":{"lines":[1,2]},"v":"General","c":[{"t":"list_item","d":4,"p":{"lines":[2,3]},"v":"<a href=\"https://arxiv.org/abs/1512.03385\">Resnet 2015</a>","c":[{"t":"list_item","d":6,"p":{"lines":[3,4]},"v":"residual learning"}]},{"t":"list_item","d":4,"p":{"lines":[4,5]},"v":"<a href=\"https://arxiv.org/abs/1607.06450\">Layer normalization 2016</a>","c":[{"t":"list_item","d":6,"p":{"lines":[5,6]},"v":"layer normalization"}]},{"t":"list_item","d":4,"p":{"lines":[6,7]},"v":"<a href=\"https://drive.google.com/file/d/1QlGDm0RTg7SJFegwRIll3Egnvczp-mWo/view?usp=drivesdk\">ResNeSt 2020</a> (split-attention)"}]},{"t":"heading","d":2,"p":{"lines":[9,10]},"v":"Action Predition"},{"t":"heading","d":2,"p":{"lines":[13,14]},"v":"<a href=\"contents/ML/ComputerVision/computer_vision.md\">Computer Vision</a>","c":[{"t":"heading","d":3,"p":{"lines":[14,15]},"v":"<strong>detection</strong>","c":[{"t":"list_item","d":5,"p":{"lines":[15,16]},"v":"R-CNN 2014"},{"t":"list_item","d":5,"p":{"lines":[16,17]},"v":"Fast R-CNN 2015"},{"t":"list_item","d":5,"p":{"lines":[17,18]},"v":"<span>Faster R-CNN 2015 üî•</span>"},{"t":"list_item","d":5,"p":{"lines":[18,19]},"v":"SSD 2016"},{"t":"list_item","d":5,"p":{"lines":[19,20]},"v":"<a href=\"contents/ML/ComputerVision/Detection/yolo.md\">YOLO (CVPR 2016)</a>"},{"t":"list_item","d":5,"p":{"lines":[20,21]},"v":"<a href=\"contents/ML/ComputerVision/Detection/yolo.md\">YOLOv2 (CVPR 2017)</a>"},{"t":"list_item","d":5,"p":{"lines":[21,22]},"v":"RetinaNet 2017"},{"t":"list_item","d":5,"p":{"lines":[22,23]},"v":"<a href=\"contents/ML/ComputerVision/Detection/yolo.md\"><span>YOLOv3 2019 üî•</span></a>"},{"t":"list_item","d":5,"p":{"lines":[23,24]},"v":"<a href=\"contents/ML/ComputerVision/Detection/yolo.md\">YOLOv4 2020</a>"},{"t":"list_item","d":5,"p":{"lines":[24,25]},"v":"RelationNet++ 2020"},{"t":"list_item","d":5,"p":{"lines":[25,26]},"v":"DETR 2020"},{"t":"list_item","d":5,"p":{"lines":[26,27]},"v":"UP-DETR 2020"}]},{"t":"heading","d":3,"p":{"lines":[28,29]},"v":"<strong>segmentation</strong>","c":[{"t":"list_item","d":5,"p":{"lines":[29,30]},"v":"Regional proposal based","c":[{"t":"list_item","d":7,"p":{"lines":[30,31]},"v":"<a href=\"contents/ML/ComputerVision/Segmentation/mask_rcnn.md\"><span>Mask R-CNN (ICCV 2017) üî•</span></a>","c":[{"t":"list_item","d":9,"p":{"lines":[31,32]},"v":"Binary ROI mask"},{"t":"list_item","d":9,"p":{"lines":[32,33]},"v":"RoIAlign"}]},{"t":"list_item","d":7,"p":{"lines":[33,34]},"v":"<a href=\"https://arxiv.org/abs/1802.05591\">LaneNet (IEEE IV 2018)</a>"},{"t":"list_item","d":7,"p":{"lines":[34,35]},"v":"<a href=\"https://ieeexplore.ieee.org/document/8953609\">Mask Scoring R-CNN (CVPR 2019)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[36,37]},"v":"RNN based","c":[{"t":"list_item","d":7,"p":{"lines":[37,38]},"v":"ReSeg"},{"t":"list_item","d":7,"p":{"lines":[38,39]},"v":"MDRNNs"}]},{"t":"list_item","d":5,"p":{"lines":[40,41]},"v":"Upsampling + Deconvolution","c":[{"t":"list_item","d":7,"p":{"lines":[41,42]},"v":"<a href=\"https://arxiv.org/abs/1605.06211v1\">FCN (CVPR 2015 &amp; TPAMI 2017)</a>"},{"t":"list_item","d":7,"p":{"lines":[42,43]},"v":"SetNet"},{"t":"list_item","d":7,"p":{"lines":[43,44]},"v":"<a href=\"https://drive.google.com/file/d/1GIOJgIe1BzChxoIWJyZq4G7EQOAE4OPY/view?usp=drivesdk\">U-net (MICCAI 2015)</a>"},{"t":"list_item","d":7,"p":{"lines":[44,45]},"v":"<a href=\"https://drive.google.com/file/d/1wIo5dLL_Sn2Bxlo4cGx4YacONw1mJU6N/view?usp=drivesdk\">FastFCN 2019</a>"}]},{"t":"list_item","d":5,"p":{"lines":[46,47]},"v":"CRF/MRF","c":[{"t":"list_item","d":7,"p":{"lines":[47,48]},"v":"<a href=\"https://drive.google.com/file/d/1X0S9WRAzMTG0hQbaysdR60prPdhPRf9E/view?usp=drivesdk\">DeepLab (ICLR 2015 &amp; ICCV 2015)</a>","c":[{"t":"list_item","d":9,"p":{"lines":[48,49]},"v":"Fully connected CRF"},{"t":"list_item","d":9,"p":{"lines":[49,50]},"v":"Atrous (Dilated) Convolution"}]},{"t":"list_item","d":7,"p":{"lines":[50,51]},"v":"<a href=\"https://drive.google.com/file/d/1oQTA8xbPvoBV0IQbOnnBsKLZVocJieQg/view?usp=drivesdk\">CRF Meet Deep Neural Networks for Semantic Segmentation 2018</a>"}]},{"t":"list_item","d":5,"p":{"lines":[52,53]},"v":"Gated-SCNN 2019"},{"t":"list_item","d":5,"p":{"lines":[54,55]},"v":"Interactive","c":[{"t":"list_item","d":7,"p":{"lines":[55,56]},"v":"<a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Interactive_Image_Segmentation_With_First_Click_Attention_CVPR_2020_paper.pdf\">Interactive Image Segmentation With First Click Attention (CVPR 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[56,57]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9156403\">F-BRS (CVPR 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[57,58]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9157733\">Interactive Object Segmentation With Inside-Outside Guidance (CVPR 2020)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[59,60]},"v":"Moving Object","c":[{"t":"list_item","d":7,"p":{"lines":[60,61]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9156405\">An End-to-End Edge Aggregation Network for Moving Object Segmentation (CVPR 2020)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[62,63]},"v":"Transparent Object","c":[{"t":"list_item","d":7,"p":{"lines":[63,64]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9156916\">Deep Polarization Cues for Transparent Object Segmentation (CVPR 2020)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[65,66]},"v":"Referring ËΩ¨‰ªã","c":[{"t":"list_item","d":7,"p":{"lines":[66,67]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9157191\">PhraseCut (CVPR 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[67,68]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9156414\">Referring Image Segmentation via Cross-Modal Progressive Comprehension (CVPR 2020)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[68,69]},"v":"Multi-Object Tracking","c":[{"t":"list_item","d":7,"p":{"lines":[69,70]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9157138\">Learning Multi-Object Tracking and Segmentation From Automatic Annotations (CVPR 2020)</a>"}]}]},{"t":"heading","d":3,"p":{"lines":[71,72]},"v":"<strong>lane detection</strong>","c":[{"t":"list_item","d":5,"p":{"lines":[72,73]},"v":"Segmentation-based","c":[{"t":"list_item","d":7,"p":{"lines":[73,74]},"v":"<a href=\"contents/ML/ComputerVision/LaneDetection/scnn.md\">SCNN (ISCA 2017)</a>"},{"t":"list_item","d":7,"p":{"lines":[74,75]},"v":"<a href=\"contents/ML/ComputerVision/LaneDetection/enet_sad.md\">ENet-SAD (ICCV 2019)</a>"},{"t":"list_item","d":7,"p":{"lines":[75,76]},"v":"CurveLanes-NAS"},{"t":"list_item","d":7,"p":{"lines":[76,77]},"v":"LaneNet 2019"},{"t":"list_item","d":7,"p":{"lines":[77,78]},"v":"RESA 2020"},{"t":"list_item","d":7,"p":{"lines":[78,79]},"v":"SUPER 2020"},{"t":"list_item","d":7,"p":{"lines":[79,80]},"v":"ERFNet-IntRA-KD 2020"}]},{"t":"list_item","d":5,"p":{"lines":[80,81]},"v":"Row-wise","c":[{"t":"list_item","d":7,"p":{"lines":[81,82]},"v":"E2E-LMD"},{"t":"list_item","d":7,"p":{"lines":[82,83]},"v":"IntRA-KD"}]},{"t":"list_item","d":5,"p":{"lines":[83,84]},"v":"Other approaches"}]},{"t":"heading","d":3,"p":{"lines":[85,86]},"v":"<strong>generation</strong>","c":[{"t":"list_item","d":5,"p":{"lines":[86,87]},"v":"image generation","c":[{"t":"list_item","d":7,"p":{"lines":[87,88]},"v":"Parzen window-based log-likelihood"},{"t":"list_item","d":7,"p":{"lines":[88,89]},"v":"<a href=\"https://arxiv.org/abs/1406.2661\">GAN 2014</a>"}]},{"t":"list_item","d":5,"p":{"lines":[89,90]},"v":"textual descriptions","c":[{"t":"list_item","d":7,"p":{"lines":[90,91]},"v":"<a href=\"https://arxiv.org/abs/1502.03044\"><span>Neural Image Caption Generation with Visual Attention üëà</span></a>"}]}]},{"t":"heading","d":3,"p":{"lines":[92,93]},"v":"<a href=\"./contents/ML/ComputerVision/OCR/ocr.md\"><strong>ocr</strong></a>","c":[{"t":"list_item","d":5,"p":{"lines":[93,94]},"v":"<a href=\"contents/ML/ComputerVision/OCR/spatial_transformer_networks.md\">Spatial Transformer Networks (NIPS 2015)</a>"},{"t":"list_item","d":5,"p":{"lines":[94,95]},"v":"<a href=\"contents/ML/ComputerVision/OCR/detecting_text_in_natural_image_with_connectionist_text_proposal_network.md\">CTPN (ECCV 2016)</a> - Text Detection"},{"t":"list_item","d":5,"p":{"lines":[95,96]},"v":"<a href=\"contents/ML/ComputerVision/OCR/crnn_ctc.md\">CRNN+CTC (TPAMI 2017)</a> - Text Recognition"},{"t":"list_item","d":5,"p":{"lines":[96,97]},"v":"<a href=\"contents/ML/ComputerVision/OCR/EAST-an_efficient_and_accurate_scene_text_detector.md\">EAST (CVPR 2017)</a> - Text Detection"},{"t":"list_item","d":5,"p":{"lines":[97,98]},"v":"<a href=\"contents/ML/ComputerVision/OCR/ASTER.md\">ASTER (TPAMI 2018)</a> - Text Recognition"},{"t":"list_item","d":5,"p":{"lines":[98,99]},"v":"<a href=\"contents/ML/ComputerVision/OCR/fots_fast_oriented_text_spotting_with_a_unified_network.md\">FOTS (CVPR 2018) üî•</a> - <strong>End2End</strong> Linux Only"},{"t":"list_item","d":5,"p":{"lines":[99,100]},"v":"<a href=\"contents/ML/ComputerVision/OCR/towards_end-to-end_text_spotting_in_natural_scenes.md\">2D atttention (ICCV 2019)</a> - <strong>End2End</strong>"},{"t":"list_item","d":5,"p":{"lines":[100,101]},"v":"<a href=\"\">Character Region Awareness for Text Detection (CVPR 2019)</a>"},{"t":"list_item","d":5,"p":{"lines":[101,102]},"v":"<a href=\"contents/ML/ComputerVision/OCR/convolutional_character_networks.md\">Convolutional Character Networks (ICCV 2019)</a>"},{"t":"list_item","d":5,"p":{"lines":[102,103]},"v":"<a href=\"contents/ML/ComputerVision/OCR/aggregation_cross-entropy_for_sequence_recognition.md\">Aggregation Cross-Entropy (CVPR 2019)</a>"},{"t":"list_item","d":5,"p":{"lines":[103,104]},"v":"<a href=\"contents/ML/ComputerVision/OCR/mask-textspotter.md\">Mask TextSpotter (TPAMI 2019)</a> - <strong>End2End</strong>"},{"t":"list_item","d":5,"p":{"lines":[104,105]},"v":"<a href=\"contents/ML/ComputerVision/OCR/end-to-end_scene_text_recognition_via_iterative_image_rectification.md\">ESIR (CVPR 2019)</a> - Text Recognition"},{"t":"list_item","d":5,"p":{"lines":[105,106]},"v":"<a href=\"contents/ML/ComputerVision/OCR/on_recognizing_texts_of_arbitrary_shapes_with_2D_self-attention.md\">SATRN (CVPR 2020)</a> - Text Recognition"},{"t":"list_item","d":5,"p":{"lines":[106,107]},"v":"<a href=\"contents/ML/ComputerVision/OCR/decoupled_attention_network_for_text_recognition.md\">DAN (AAAI 2020)</a> - Text Recognition"},{"t":"list_item","d":5,"p":{"lines":[107,108]},"v":"<a href=\"contents/ML/ComputerVision/OCR/why_you_should_try_the_real_data_for_the_scene_text_recognition.md\">Yet Another Text Recognizer (ArXiv 2021)</a> - Text Recognition"},{"t":"list_item","d":5,"p":{"lines":[108,109]},"v":"<a href=\"contents/ML/ComputerVision/OCR/ABCNet_real-time_scene_text_spotting_with_adaptive_bezier-curve_network.md\">ABCNet (CVPR 2020)</a> - <strong>End2End</strong> Linux Only"},{"t":"list_item","d":5,"p":{"lines":[109,110]},"v":"<a href=\"contents/ML/ComputerVision/OCR/abcnet_v2.md\">ABCNet v2 (CVPR 2021) üî•</a> - <strong>End2End</strong> Linux Only"},{"t":"list_item","d":5,"p":{"lines":[110,111]},"v":"<a href=\"contents/ML/ComputerVision/OCR/pgnet.md\">PGNet (AAAI 2021) üî•</a> - <strong>End2End</strong>"}]},{"t":"heading","d":3,"p":{"lines":[113,114]},"v":"<strong>video analysis</strong>","c":[{"t":"bullet_list","d":4,"p":{"lines":[114,127]},"v":"","c":[{"t":"list_item","d":5,"p":{"lines":[114,115]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/video_description.md\">Video Description, Video Reasoning</a>","c":[{"t":"list_item","d":7,"p":{"lines":[115,116]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/describing_videos_by_exploiting_temporal_structure.md\">Describing Videos by Exploiting Temporal Structure (ICCV 2015)</a>"},{"t":"list_item","d":7,"p":{"lines":[116,117]},"v":"<a href=\"\">Localizing Moments in Video with Natural Language (ICCV 2017)</a>"},{"t":"list_item","d":7,"p":{"lines":[117,118]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/grounded_video_description.md\"><span>Grounded Video Description (CVPR 2019) üî•</span></a>"},{"t":"list_item","d":7,"p":{"lines":[118,119]},"v":"<a href=\"video_relationship_reasoning_using_gated_spatio-temporal_energy_graph.md\">Video Relationship Reasoning Using Gated Spatio-Temporal Energy Graph (GSTEG, CVPR 2019)</a> - CRF"},{"t":"list_item","d":7,"p":{"lines":[119,120]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/adversarial_inference_for_multi-sentence_video_description.md\">Adversarial Inference for Multi-Sentence Video Description (CVPR 2019)</a> - adversarial inference"},{"t":"list_item","d":7,"p":{"lines":[120,121]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/delving_deeper_into_the_decoder_for_video_captioning.md\">Delving Deeper into the Decoder for Video Captioning (ECAI 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[121,122]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/identity-aware_multi-sentence_video_description.md\">Identity-Aware Multi-Sentence Video Description (ECCV 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[122,123]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/describing_unseen_videos_via_multi_modal_cooperative_dialog_agents.md\">Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents (ECCV 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[123,124]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/exploiting_visual_semantic_reasoning_for_video-text_retrieval.md\"><span>Exploiting Visual Semantic Reasoning for Video-Text Retrieval (IJCAL 2020) üî•</span></a>"},{"t":"list_item","d":7,"p":{"lines":[124,125]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/fine-grained_video-text_retrieval_with_hierarchical_graph_reasoning.md\">Fine-Grained Video-Text Retrieval With Hierarchical Graph Reasoning (CVPR 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[125,126]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/a_hierarchical_reasoning_graph_neural_network_for_the_automatic_scoring_of_answer_transcriptions_in_video_job_interviews.md\">A Hierarchical Reasoning Graph Neural Network for The Automatic Scoring of Answer Transcriptions in Video Job Interviews (ArXiv 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[126,127]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/reasoning_with_heterogeneous_graph_alignment_for_video_question_answering.md\">Reasoning with Heterogeneous Graph Alignment for Video Question Answering (AAAI 2020)</a>"}]}]},{"t":"bullet_list","d":4,"p":{"lines":[129,154]},"v":"","c":[{"t":"list_item","d":5,"p":{"lines":[129,130]},"v":"3D Reconstruction","c":[{"t":"list_item","d":7,"p":{"lines":[130,131]},"v":"<a href=\"https://www.matthewtancik.com/nerf\"><span>NeRF (ECCV 2020) üî•</span></a>"}]},{"t":"list_item","d":5,"p":{"lines":[131,132]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/temporal_action_detection.md\">Temporal Action Detection</a>","c":[{"t":"list_item","d":7,"p":{"lines":[132,133]},"v":"frame-based","c":[{"t":"list_item","d":9,"p":{"lines":[133,134]},"v":"<a href=\"\">CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos (CVPR 2017)</a>"},{"t":"list_item","d":9,"p":{"lines":[134,135]},"v":"<a href=\"\">TAG: Temporal Action Detection with Structured Segment Networks (ICCV 2017)</a>"},{"t":"list_item","d":9,"p":{"lines":[135,136]},"v":"<a href=\"2017https://zhuanlan.zhihu.com/p/84956905\">I3D (CVPR)</a>"},{"t":"list_item","d":9,"p":{"lines":[136,137]},"v":"<a href=\"https://arxiv.org/pdf/1812.03982.pdf\">SlowFastNet (ICCV 2019)</a>"}]},{"t":"list_item","d":7,"p":{"lines":[137,138]},"v":"proposal-based","c":[{"t":"list_item","d":9,"p":{"lines":[138,139]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/temporal_action_localization_in_untrimmed_videos_via_multi-stage_cnns.md\">SCNN Segment-CNN (CVPR 2016)</a>"},{"t":"list_item","d":9,"p":{"lines":[139,140]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/TURN-TAP.md\">TURN TAP (ICCV 2017)</a>"},{"t":"list_item","d":9,"p":{"lines":[140,141]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/CBR_cascaded_boundary_regression.md\">CBR: Cascaded Boundary Regression (BMVC 2017)</a>"}]},{"t":"list_item","d":7,"p":{"lines":[141,142]},"v":"attention-based","c":[{"t":"list_item","d":9,"p":{"lines":[142,143]},"v":"<a href=\"https://zhuanlan.zhihu.com/p/357848386\">TimeSformer</a>"}]},{"t":"list_item","d":7,"p":{"lines":[143,144]},"v":"self-supervised","c":[{"t":"list_item","d":9,"p":{"lines":[144,145]},"v":"<a href=\"https://zhuanlan.zhihu.com/p/250477141\">Visual&amp;CBT</a>"}]},{"t":"list_item","d":7,"p":{"lines":[145,146]},"v":"online action detection (ÂºÇÂ∏∏‰∫ã‰ª∂ÁõëÊµã)","c":[{"t":"list_item","d":9,"p":{"lines":[146,147]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/online_action_detection.md\">Online Action Detection (ECCV 2016)</a>"},{"t":"list_item","d":9,"p":{"lines":[147,148]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/RED.md\">RED: Reinforced Encoder-Decoder Networksfor Action Anticipation (BMVC 2017)</a>"},{"t":"list_item","d":9,"p":{"lines":[148,149]},"v":"<a href=\"\">Online Action Detection in Untrimmed, Streaming Videos (ECCV 2018)</a>"},{"t":"list_item","d":9,"p":{"lines":[149,150]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/temporal_recurrent_networks_for_online_action_detection.md\">Temporal Recurrent Networks for Online Action Detection (ICCV 2019)</a>"}]},{"t":"list_item","d":7,"p":{"lines":[151,152]},"v":"multiple actors","c":[{"t":"list_item","d":9,"p":{"lines":[152,153]},"v":"<a href=\"https://drive.google.com/file/d/1ta4UmPSjyjzC7mJCp2AldvMs-uh42NMU/view?usp=drivesdk\">Action Understandingwith Multiple Classes of Actors</a>"},{"t":"list_item","d":9,"p":{"lines":[153,154]},"v":"<a href=\"https://www.crcv.ucf.edu/wp-content/uploads/2020/12/Projects_Single-shot-actor-action-detection-in-videos.pdf\">SSA2D</a>"}]}]}]}]}]},{"t":"heading","d":2,"p":{"lines":[155,156]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/nature_language_processing.md\">Natural Language Processing</a>","c":[{"t":"list_item","d":4,"p":{"lines":[156,157]},"v":"traditional methods","c":[{"t":"list_item","d":6,"p":{"lines":[157,158]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/word2vec.md\">word2vec (‚ÄéGoogle 2013)</a>"}]},{"t":"list_item","d":4,"p":{"lines":[158,159]},"v":"full sentence translation","c":[{"t":"list_item","d":6,"p":{"lines":[159,160]},"v":"seq2seq"},{"t":"list_item","d":6,"p":{"lines":[160,161]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/a_neural_probabilistic_language_model.md\">A Neural Probabilistic Language Model (JMRL 2003)</a>","c":[{"t":"list_item","d":8,"p":{"lines":[161,162]},"v":"word embeddings"}]},{"t":"list_item","d":6,"p":{"lines":[162,163]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/self_attentive_sentence_embedding.md\">A Structured Self-Attentive Sentence Embedding (ICLR 2017)</a>","c":[{"t":"list_item","d":8,"p":{"lines":[163,164]},"v":"Self-Attention"}]},{"t":"list_item","d":6,"p":{"lines":[164,165]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/transformer.md\">Transformer (NIPS 2017)</a>","c":[{"t":"list_item","d":8,"p":{"lines":[165,166]},"v":"Multi-Head Self-Attention"},{"t":"list_item","d":8,"p":{"lines":[166,167]},"v":"Positional Encoding"},{"t":"list_item","d":8,"p":{"lines":[167,168]},"v":"Scaled Dot-Product Attention"}]},{"t":"list_item","d":6,"p":{"lines":[168,169]},"v":"<a href=\"https://arxiv.org/abs/1807.03819\">Universal transformer (ICLR 2019)</a>","c":[{"t":"list_item","d":8,"p":{"lines":[169,170]},"v":"Transition Function"},{"t":"list_item","d":8,"p":{"lines":[170,171]},"v":"Recurrent Mechanism"}]},{"t":"list_item","d":6,"p":{"lines":[171,172]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/bert.md\"><span>BERT (Arxiv 2018) üëà</span></a>"}]},{"t":"list_item","d":4,"p":{"lines":[172,173]},"v":"Simultaneous Translation","c":[{"t":"list_item","d":6,"p":{"lines":[173,174]},"v":"<a href=\"https://drive.google.com/file/d/14_1-FOfAf-HZv-y1AHwKpGPpytfZlFcR/view?usp=drivesdk\"><span>STACL (ACL 2019) üëà</span></a>"}]},{"t":"list_item","d":4,"p":{"lines":[174,175]},"v":"Machine Translation","c":[{"t":"list_item","d":6,"p":{"lines":[175,176]},"v":"<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0885230806000325\">Continuous space language models (CSL 2007)</a>"},{"t":"list_item","d":6,"p":{"lines":[176,177]},"v":"<a href=\"https://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf\">Statistical Language Models Based on Neural Networks 2012</a>"}]}]},{"t":"heading","d":2,"p":{"lines":[179,180]},"v":"<a href=\"contents/ML/GraphTheory/graph_theory.md\">Graph Theory</a>","c":[{"t":"list_item","d":4,"p":{"lines":[180,181]},"v":"traditional methods","c":[{"t":"list_item","d":6,"p":{"lines":[181,182]},"v":"LINE"},{"t":"list_item","d":6,"p":{"lines":[182,183]},"v":"TADW"}]},{"t":"list_item","d":4,"p":{"lines":[183,184]},"v":"<a href=\"contents/ML/GraphTheory/graph_embedding.md\">graph embedding</a>","c":[{"t":"list_item","d":6,"p":{"lines":[184,185]},"v":"<a href=\"contents/ML/GraphTheory/word2vec.md\">word2vec (ArXiv 2013)</a> - SkipGram"},{"t":"list_item","d":6,"p":{"lines":[185,186]},"v":"<a href=\"contents/ML/GraphTheory/deepwalk.md\">DeepWalk (KDD 2014)</a> - graph + word2vec"},{"t":"list_item","d":6,"p":{"lines":[186,187]},"v":"<a href=\"contents/ML/GraphTheory/node2vec.md\">node2vec (KDD 2016)</a>"},{"t":"list_item","d":6,"p":{"lines":[187,188]},"v":"<a href=\"contents/ML/GraphTheory/item2vec.md\">item2vec (MLSP 2016)</a>"},{"t":"list_item","d":6,"p":{"lines":[188,189]},"v":"<a href=\"\">Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification (AAAI 2020)</a>"},{"t":"list_item","d":6,"p":{"lines":[189,190]},"v":"<a href=\"\">GraphZoom (ICLR 2020)</a>"}]},{"t":"list_item","d":4,"p":{"lines":[191,192]},"v":"<a href=\"contents/ML/GraphTheory/graph_neural_network.md\">graph neural networks</a>","c":[{"t":"list_item","d":6,"p":{"lines":[192,193]},"v":"convolutional network","c":[{"t":"list_item","d":8,"p":{"lines":[193,194]},"v":"spectral","c":[{"t":"list_item","d":10,"p":{"lines":[194,195]},"v":"<a href=\"contents/ML/GraphTheory/spectral_networks_and_deep_locally_connected_networks_on_graphs.md\">Spectral networks and locally connected networks on graphs (ICLR 2014, ÊúÄÊó©ÁöÑÈ¢ëË∞±ÂõæÁ•ûÁªèÁΩëÁªú)</a>"},{"t":"list_item","d":10,"p":{"lines":[195,196]},"v":"<a href=\"contents/ML/GraphTheory/deep_convolutional_networks_on_graph-structured_data.md\">Deep convolutional networks on graph-structured data (CoRR 2015)</a>"},{"t":"list_item","d":10,"p":{"lines":[196,197]},"v":"<a href=\"contents/ML/GraphTheory/convolutional_neural_networks_on_graphs_with_fast_localized_spectral_filtering.md\">Convolutional Neural Networks on Graphswith Fast Localized Spectral Filtering (Chebyshev expansion, NIPS 2016)</a>"},{"t":"list_item","d":10,"p":{"lines":[197,198]},"v":"<a href=\"contents/ML/GraphTheory/semi-supervised_classification_with_graph_convolutional_networks.md\"><span>GCN (ICLR 2017) üî•</span></a>"},{"t":"list_item","d":10,"p":{"lines":[198,199]},"v":"AGCN"},{"t":"list_item","d":10,"p":{"lines":[199,200]},"v":"GGP"}]},{"t":"list_item","d":8,"p":{"lines":[200,201]},"v":"spatial","c":[{"t":"list_item","d":10,"p":{"lines":[201,202]},"v":"<a href=\"contents/ML/GraphTheory/convolutional_networks_on_graphs_for_learning_molecular_fingerprints.md\">Convolutional networks on graphs for learning molecular fingerprints (NIPS 2015)</a>"},{"t":"list_item","d":10,"p":{"lines":[202,203]},"v":"<a href=\"contents/ML/GraphTheory/diffusion-convolutional_neural_networks.md\">Diffusion-convolutional neural networks (IANIPS 2016)</a>"},{"t":"list_item","d":10,"p":{"lines":[203,204]},"v":"Neural FPs"},{"t":"list_item","d":10,"p":{"lines":[204,205]},"v":"PATCHY-SAN"},{"t":"list_item","d":10,"p":{"lines":[205,206]},"v":"MoNet 2017"},{"t":"list_item","d":10,"p":{"lines":[206,207]},"v":"<a href=\"contents/ML/GraphTheory/GraphSAGE.md\">GraphSAGE (NIPS 2017)</a>"},{"t":"list_item","d":10,"p":{"lines":[207,208]},"v":"SACNN 2018"},{"t":"list_item","d":10,"p":{"lines":[208,209]},"v":"DCNN"}]}]},{"t":"list_item","d":6,"p":{"lines":[209,210]},"v":"attention-based network","c":[{"t":"list_item","d":8,"p":{"lines":[210,211]},"v":"<a href=\"\">NLNN</a>"},{"t":"list_item","d":8,"p":{"lines":[211,212]},"v":"<a href=\"contents/ML/GraphTheory/one-shot_imitation_learning.md\">One-Shot Imitation Learning (neighborhood attention, NIPS 2017)</a>"},{"t":"list_item","d":8,"p":{"lines":[212,213]},"v":"<a href=\"contents/ML/GraphTheory/graph_attention_networks.md\"><span>GAT (self-attention, ICLR 2018) üî•</span></a>"}]},{"t":"list_item","d":6,"p":{"lines":[214,215]},"v":"spatial-temporal graph"},{"t":"list_item","d":6,"p":{"lines":[215,216]},"v":"hierarchical graph","c":[{"t":"list_item","d":8,"p":{"lines":[216,217]},"v":"<a href=\"contents/ML/GraphTheory/pairnorm.md\">PairNorm (ICLR 2020)</a>"},{"t":"list_item","d":8,"p":{"lines":[217,218]},"v":"<a href=\"contents/ML/GraphTheory/subgraph_neural_networks.md\">Subgraph Neural Networks (NIPS 2020)</a>"}]},{"t":"list_item","d":6,"p":{"lines":[219,220]},"v":"relational reasoning","c":[{"t":"list_item","d":8,"p":{"lines":[220,221]},"v":"<a href=\"contents/ML/GraphTheory/a_simple_neural_network_module_for_relational_reasoning.md\">A simple neural network module for relational reasoning (NIPS 2017)</a>"},{"t":"list_item","d":8,"p":{"lines":[221,222]},"v":"<a href=\"contents/ML/GraphTheory/modeling_relational_data_with_graph_convolutional_networks.md\">Relational-GCN (ESWC 2018)</a>"},{"t":"list_item","d":8,"p":{"lines":[222,223]},"v":"<a href=\"contents/ML/GraphTheory/VAIN-attentional_multi-agent_predictive_modeling.md\">VAIN (NIPS 2017)</a>"},{"t":"list_item","d":8,"p":{"lines":[223,224]},"v":"<a href=\"contents/ML/GraphTheory/composition-based_multi-relational_graph_convolutional_networks.md\">CompGCN (ICLR 2020)</a>"},{"t":"list_item","d":8,"p":{"lines":[224,225]},"v":"<a href=\"contents/ML/GraphTheory/temporal_knowledge_graph_reasoning_based_on_evolutional_representation_learning.md\">Temporal Knowledge Graph Reasoning Based on Evolutional Representation Learning (ArXiv 2021)</a>"}]},{"t":"list_item","d":6,"p":{"lines":[226,227]},"v":"image segmentation","c":[{"t":"list_item","d":8,"p":{"lines":[227,228]},"v":"<a href=\"contents/ML/GraphTheory/zero-shot_video_object_segmentation_via_attentive_graph_neural_networks.md\">AGNN (ICCV 2019)</a> <a href=\"https://github.com/carrierlxk/AGNN\"><img src=\"contents/GitHub.png\" alt=\"\"></a>"}]},{"t":"list_item","d":6,"p":{"lines":[229,230]},"v":"<a href=\"contents/ML/GraphTheory/heterogeneous_graph_embedding.md\">Heterogeneity ÂºÇË¥®ÊÄß</a>","c":[{"t":"list_item","d":8,"p":{"lines":[230,231]},"v":"<a href=\"contents/ML/GraphTheory/metapath2vec.md\">metapath2vec (KDD 2017)</a>"},{"t":"list_item","d":8,"p":{"lines":[231,232]},"v":"<a href=\"hindroid-an-intelligent_android_malware_detection_system_based_on_structured.md\">Hindroid (KDD 2017)</a>"},{"t":"list_item","d":8,"p":{"lines":[232,233]},"v":"<a href=\"contents/ML/GraphTheory/metagraph2vec.md\">metagraph2vec (PAKDD 2018)</a>"},{"t":"list_item","d":8,"p":{"lines":[233,234]},"v":"<a href=\"contents/ML/GraphTheory/heterogeneous_graph_neural_network.md\">Heterogeneous graph neural network (KDD 2919)</a>"},{"t":"list_item","d":8,"p":{"lines":[234,235]},"v":"<a href=\"contents/ML/GraphTheory/heterogeneous_graph_attention_network.md\">Heterogeneous graph attention network (HAN, WWW 2019)</a>"},{"t":"list_item","d":8,"p":{"lines":[235,236]},"v":"<a href=\"contents/ML/GraphTheory/gatne.md\">GATNE (KDD 2019)</a>"}]}]}]},{"t":"heading","d":2,"p":{"lines":[238,239]},"v":"Outlier Detection","c":[{"t":"list_item","d":4,"p":{"lines":[239,240]},"v":"one class classification"},{"t":"list_item","d":4,"p":{"lines":[240,241]},"v":"open set recognition"}]},{"t":"heading","d":2,"p":{"lines":[243,244]},"v":"Point Cloud"},{"t":"heading","d":2,"p":{"lines":[246,247]},"v":"<a href=\"contents/ML/Recommendation/recommendation.md\">Recommendation</a>","c":[{"t":"list_item","d":4,"p":{"lines":[247,248]},"v":"<a href=\"contents/ML/Recommendation/lira.md\">LIRA 1998</a>"},{"t":"list_item","d":4,"p":{"lines":[248,249]},"v":"<a href=\"https://drive.google.com/file/d/16bSJlpzmGmxAgh-U_zMyl1QJvqhcY_vO/view?usp=drivesdk\">SimRank 2002</a>"},{"t":"list_item","d":4,"p":{"lines":[249,250]},"v":"<a href=\"https://drive.google.com/file/d/1zPbx8cq_pOqljk3xCmJ0HsafqK7dFP6H/view?usp=drivesdk\">Matrix Factorization 2009</a>"},{"t":"list_item","d":4,"p":{"lines":[250,251]},"v":"<a href=\"https://drive.google.com/file/d/1LB-iSCGi0UDCZBsDgZpFA-4P9U8NJ8ro/view?usp=drivesdk\">WSABIE rank loss 2011</a>"},{"t":"list_item","d":4,"p":{"lines":[251,252]},"v":"<a href=\"https://drive.google.com/file/d/1FTxR2qfzXrP2dui53DGM19_fvEUh9BVj/view?usp=drivesdk\">XGBoost 2016</a>"},{"t":"list_item","d":4,"p":{"lines":[252,253]},"v":"<a href=\"contents/ML/Recommendation/deep_neural_networks_for_youtube_ecommendations.md\">Deep Neural Networks for YouTube Recommendations 2016</a>"},{"t":"list_item","d":4,"p":{"lines":[253,254]},"v":"<a href=\"contents/ML/Recommendation/dlrm.md\">DLRM 2019</a>"},{"t":"list_item","d":4,"p":{"lines":[254,255]},"v":"<a href=\"https://drive.google.com/file/d/1yLhp2yUVHHmbqtWOxVdLUGoTSW4Odjrm/view?usp=drivesdk\">Two-tower model 2020</a>"}]},{"t":"heading","d":2,"p":{"lines":[258,259]},"v":"<a href=\"contents/ML/UnsupervisedLearning/unsupervised_learning.md\">Unsupervised Learning</a>","c":[{"t":"table","d":3,"p":{"lines":[260,264]},"v":"","c":[{"t":"thead","d":4,"p":{"lines":[260,261]},"v":"","c":[{"t":"th","d":6,"p":{"lines":[260,261]},"v":""},{"t":"th","d":6,"p":{"lines":[260,261]},"v":"With Teacher"},{"t":"th","d":6,"p":{"lines":[260,261]},"v":"Without Teacher"}]},{"t":"tbody","d":4,"p":{"lines":[262,264]},"v":"","c":[{"t":"tr","d":5,"p":{},"v":"","c":[{"t":"td","d":6,"p":{},"v":"<strong>Active</strong>"},{"t":"td","d":6,"p":{},"v":"Reinforcement Learning/Active Learning"},{"t":"td","d":6,"p":{},"v":"Intrinsic Motivation/Exploration"}]},{"t":"tr","d":5,"p":{},"v":"","c":[{"t":"td","d":6,"p":{},"v":"<strong>Passive</strong>"},{"t":"td","d":6,"p":{},"v":"Supervised Learning"},{"t":"td","d":6,"p":{},"v":"Unsupervised Learning"}]}]}]},{"t":"heading","d":3,"p":{"lines":[265,266]},"v":"<a href=\"contents/ML/UnsupervisedLearning/active_learning.md\">active learning</a>"},{"t":"heading","d":3,"p":{"lines":[267,268]},"v":"<a href=\"contents/ML/UnsupervisedLearning/representation_learning.md\">representation learning</a>","c":[{"t":"list_item","d":5,"p":{"lines":[268,269]},"v":"Colorization 2016"},{"t":"list_item","d":5,"p":{"lines":[269,270]},"v":"<a href=\"https://arxiv.org/abs/1703.10593\">Cycle-GAN (ICCV 2017)</a>"},{"t":"list_item","d":5,"p":{"lines":[270,271]},"v":"Unsupervised word translation 2018"},{"t":"list_item","d":5,"p":{"lines":[271,272]},"v":"Deep clustering 2018"},{"t":"list_item","d":5,"p":{"lines":[272,273]},"v":"<a href=\"https://drive.google.com/file/d/1FdfNcJDI0Y4QbVRuW79-c9iSvd8VErGp/view\">XLM-R 2020</a>"},{"t":"list_item","d":5,"p":{"lines":[273,274]},"v":"<a href=\"contents/ML/UnsupervisedLearning/tabnet.md\">TabNet 2020</a>"},{"t":"list_item","d":5,"p":{"lines":[274,275]},"v":"<a href=\"contents/ML/UnsupervisedLearning/on_mutual_infomration_maximization_for_representation_learning.md\">On mutual infomration maximization for representation learning 2020</a>"}]},{"t":"heading","d":3,"p":{"lines":[277,278]},"v":"autoregressive networks","c":[{"t":"list_item","d":5,"p":{"lines":[278,279]},"v":"WaveNets 2016"},{"t":"list_item","d":5,"p":{"lines":[279,280]},"v":"PixelRNN 2016"}]}]}]})</script>
</body>
</html>
