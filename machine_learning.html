<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.14.4/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@6.7.0"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.14.4"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.14.4/dist/index.umd.min.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
        const markmap = getMarkmap();
        window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
      })(() => window.markmap,null,{"type":"heading","depth":0,"payload":{"lines":[0,1]},"content":"Machine Learning","children":[{"type":"heading","depth":1,"payload":{"lines":[1,2]},"content":"General","children":[{"type":"list_item","depth":2,"payload":{"lines":[2,3]},"content":"<a href=\"https://arxiv.org/abs/1512.03385\">Resnet 2015</a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[3,4]},"content":"residual learning"}]},{"type":"list_item","depth":2,"payload":{"lines":[4,5]},"content":"<a href=\"https://arxiv.org/abs/1607.06450\">Layer normalization 2016</a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[5,6]},"content":"layer normalization"}]},{"type":"list_item","depth":2,"payload":{"lines":[6,7]},"content":"<a href=\"https://drive.google.com/file/d/1QlGDm0RTg7SJFegwRIll3Egnvczp-mWo/view?usp=drivesdk\">ResNeSt 2020</a> (split-attention)"}]},{"type":"heading","depth":1,"payload":{"lines":[9,10]},"content":"<a href=\"ComputerVision/computer_vision.md\">Computer Vision</a>","children":[{"type":"heading","depth":2,"payload":{"lines":[10,11]},"content":"<a href=\"ComputerVision/Detection/detection.md\">detection</a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[11,12]},"content":"R-CNN 2014","children":[{"type":"list_item","depth":4,"payload":{"lines":[12,13]},"content":"two-stages: pretrained CNN (one forward pass for each object proposal) + SVM"}]},{"type":"list_item","depth":3,"payload":{"lines":[13,14]},"content":"<a href=\"ComputerVision/Detection/fast-rcnn.md\">Fast R-CNN 2015</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[14,15]},"content":"SPPnet (one forward pass for all proposals) + RoI pooling layer"}]},{"type":"list_item","depth":3,"payload":{"lines":[15,16]},"content":"<a href=\"ComputerVision/Detection/faster-rcnn.md\">Faster R-CNN 2015</a> <span> üî•</span>","children":[{"type":"list_item","depth":4,"payload":{"lines":[16,17]},"content":"region proposal network (RPN) to replace RoI pooling"}]},{"type":"list_item","depth":3,"payload":{"lines":[17,18]},"content":"SSD 2016"},{"type":"list_item","depth":3,"payload":{"lines":[18,19]},"content":"<a href=\"ComputerVision/Detection/yolo.md\">YOLO (CVPR 2016)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[19,20]},"content":"<a href=\"ComputerVision/Detection/yolo.md\">YOLOv2 (CVPR 2017)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[20,21]},"content":"<a href=\"ComputerVision/Detection/RetinaNet.md\">RetinaNet 2017</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[21,22]},"content":"Focal loss (solve data imbalance) + Feature Pyramid Network + RPN"}]},{"type":"list_item","depth":3,"payload":{"lines":[22,23]},"content":"<a href=\"ComputerVision/Detection/yolo.md\"><span>YOLOv3 2019 üî•</span></a>"},{"type":"list_item","depth":3,"payload":{"lines":[23,24]},"content":"<a href=\"ComputerVision/Detection/yolo.md\">YOLOv4 2020</a>"},{"type":"list_item","depth":3,"payload":{"lines":[24,25]},"content":"RelationNet++ 2020"},{"type":"list_item","depth":3,"payload":{"lines":[25,26]},"content":"DETR 2020"},{"type":"list_item","depth":3,"payload":{"lines":[26,27]},"content":"UP-DETR 2020"}]},{"type":"heading","depth":2,"payload":{"lines":[28,29]},"content":"<a href=\"ComputerVision/OneFewShot/one-or-few-shot-object-detection.md\">one/few shot object detection</a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[29,30]},"content":"<a href=\"\">OS2D (ECCV 2020)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[30,31]},"content":"<a href=\"\">One-Shot Object Detection without Fine-Tuning (2020)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[31,32]},"content":"<a href=\"\">Quasi-Dense Similarity Learning for Multiple Object Tracking (CVPR 2021)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[32,33]},"content":"<a href=\"\">Meta Faster R-CNN (AAAI 2022)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[33,34]},"content":"<a href=\"\">Semantic-Aligned Fusion Transformer for One-Shot Object Detection (CVPR 2022)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[34,35]},"content":"<a href=\"\">One-Shot General Object Localization (arxiv 2022)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[35,36]},"content":"<a href=\"\">Balanced and Hierarchical Relation Learning for One-Shot Object Detection (CVPR 2022)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[36,37]},"content":"<a href=\"\">Simple Open-Vocabulary Object Detection with Vision Transformers (ECCV 2022)</a>"}]},{"type":"heading","depth":2,"payload":{"lines":[38,39]},"content":"<strong>segmentation</strong>","children":[{"type":"list_item","depth":3,"payload":{"lines":[39,40]},"content":"Regional proposal based","children":[{"type":"list_item","depth":4,"payload":{"lines":[40,41]},"content":"<a href=\"ComputerVision/Segmentation/mask_rcnn.md\"><span>Mask R-CNN (ICCV 2017) üî•</span></a>","children":[{"type":"list_item","depth":5,"payload":{"lines":[41,42]},"content":"Binary ROI mask"},{"type":"list_item","depth":5,"payload":{"lines":[42,43]},"content":"RoIAlign"}]},{"type":"list_item","depth":4,"payload":{"lines":[43,44]},"content":"<a href=\"https://arxiv.org/abs/1802.05591\">LaneNet (IEEE IV 2018)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[44,45]},"content":"<a href=\"https://ieeexplore.ieee.org/document/8953609\">Mask Scoring R-CNN (CVPR 2019)</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[46,47]},"content":"RNN based","children":[{"type":"list_item","depth":4,"payload":{"lines":[47,48]},"content":"ReSeg"},{"type":"list_item","depth":4,"payload":{"lines":[48,49]},"content":"MDRNNs"}]},{"type":"list_item","depth":3,"payload":{"lines":[50,51]},"content":"Upsampling + Deconvolution","children":[{"type":"list_item","depth":4,"payload":{"lines":[51,52]},"content":"<a href=\"https://arxiv.org/abs/1605.06211v1\">FCN (CVPR 2015 &amp; TPAMI 2017)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[52,53]},"content":"SetNet"},{"type":"list_item","depth":4,"payload":{"lines":[53,54]},"content":"<a href=\"https://drive.google.com/file/d/1GIOJgIe1BzChxoIWJyZq4G7EQOAE4OPY/view?usp=drivesdk\">U-net (MICCAI 2015)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[54,55]},"content":"<a href=\"https://drive.google.com/file/d/1wIo5dLL_Sn2Bxlo4cGx4YacONw1mJU6N/view?usp=drivesdk\">FastFCN 2019</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[56,57]},"content":"CRF/MRF","children":[{"type":"list_item","depth":4,"payload":{"lines":[57,58]},"content":"<a href=\"https://drive.google.com/file/d/1X0S9WRAzMTG0hQbaysdR60prPdhPRf9E/view?usp=drivesdk\">DeepLab (ICLR 2015 &amp; ICCV 2015)</a>","children":[{"type":"list_item","depth":5,"payload":{"lines":[58,59]},"content":"Fully connected CRF"},{"type":"list_item","depth":5,"payload":{"lines":[59,60]},"content":"Atrous (Dilated) Convolution"}]},{"type":"list_item","depth":4,"payload":{"lines":[60,61]},"content":"<a href=\"https://drive.google.com/file/d/1oQTA8xbPvoBV0IQbOnnBsKLZVocJieQg/view?usp=drivesdk\">CRF Meet Deep Neural Networks for Semantic Segmentation 2018</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[62,63]},"content":"Gated-SCNN 2019"},{"type":"list_item","depth":3,"payload":{"lines":[64,65]},"content":"Interactive","children":[{"type":"list_item","depth":4,"payload":{"lines":[65,66]},"content":"<a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Interactive_Image_Segmentation_With_First_Click_Attention_CVPR_2020_paper.pdf\">Interactive Image Segmentation With First Click Attention (CVPR 2020)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[66,67]},"content":"<a href=\"https://ieeexplore.ieee.org/document/9156403\">F-BRS (CVPR 2020)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[67,68]},"content":"<a href=\"https://ieeexplore.ieee.org/document/9157733\">Interactive Object Segmentation With Inside-Outside Guidance (CVPR 2020)</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[69,70]},"content":"Moving Object","children":[{"type":"list_item","depth":4,"payload":{"lines":[70,71]},"content":"<a href=\"https://ieeexplore.ieee.org/document/9156405\">An End-to-End Edge Aggregation Network for Moving Object Segmentation (CVPR 2020)</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[72,73]},"content":"Transparent Object","children":[{"type":"list_item","depth":4,"payload":{"lines":[73,74]},"content":"<a href=\"https://ieeexplore.ieee.org/document/9156916\">Deep Polarization Cues for Transparent Object Segmentation (CVPR 2020)</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[75,76]},"content":"Referring ËΩ¨‰ªã","children":[{"type":"list_item","depth":4,"payload":{"lines":[76,77]},"content":"<a href=\"https://ieeexplore.ieee.org/document/9157191\">PhraseCut (CVPR 2020)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[77,78]},"content":"<a href=\"https://ieeexplore.ieee.org/document/9156414\">Referring Image Segmentation via Cross-Modal Progressive Comprehension (CVPR 2020)</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[78,79]},"content":"Multi-Object Tracking","children":[{"type":"list_item","depth":4,"payload":{"lines":[79,80]},"content":"<a href=\"https://ieeexplore.ieee.org/document/9157138\">Learning Multi-Object Tracking and Segmentation From Automatic Annotations (CVPR 2020)</a>"}]}]},{"type":"heading","depth":2,"payload":{"lines":[81,82]},"content":"<a href=\"ComputerVision/LaneDetection/lane_detection.md\"><strong>lane detection</strong></a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[82,83]},"content":"Segmentation-based","children":[{"type":"list_item","depth":4,"payload":{"lines":[83,84]},"content":"<a href=\"ComputerVision/LaneDetection/scnn.md\">SCNN (ISCA 2017)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[84,85]},"content":"<a href=\"ComputerVision/LaneDetection/enet_sad.md\">ENet-SAD (ICCV 2019)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[85,86]},"content":"CurveLanes-NAS"},{"type":"list_item","depth":4,"payload":{"lines":[86,87]},"content":"LaneNet 2019"},{"type":"list_item","depth":4,"payload":{"lines":[87,88]},"content":"RESA 2020"},{"type":"list_item","depth":4,"payload":{"lines":[88,89]},"content":"SUPER 2020"},{"type":"list_item","depth":4,"payload":{"lines":[89,90]},"content":"ERFNet-IntRA-KD 2020"}]},{"type":"list_item","depth":3,"payload":{"lines":[90,91]},"content":"Row-wise","children":[{"type":"list_item","depth":4,"payload":{"lines":[91,92]},"content":"E2E-LMD"},{"type":"list_item","depth":4,"payload":{"lines":[92,93]},"content":"IntRA-KD"}]},{"type":"list_item","depth":3,"payload":{"lines":[93,94]},"content":"Other approaches"}]},{"type":"heading","depth":2,"payload":{"lines":[95,96]},"content":"<strong>generation</strong>","children":[{"type":"list_item","depth":3,"payload":{"lines":[96,97]},"content":"image generation","children":[{"type":"list_item","depth":4,"payload":{"lines":[97,98]},"content":"Parzen window-based log-likelihood"},{"type":"list_item","depth":4,"payload":{"lines":[98,99]},"content":"<a href=\"https://arxiv.org/abs/1406.2661\">GAN 2014</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[99,100]},"content":"textual descriptions","children":[{"type":"list_item","depth":4,"payload":{"lines":[100,101]},"content":"<a href=\"https://arxiv.org/abs/1502.03044\"><span>Neural Image Caption Generation with Visual Attention üëà</span></a>"}]}]},{"type":"heading","depth":2,"payload":{"lines":[102,103]},"content":"<a href=\"./ComputerVision/OCR/ocr.md\"><strong>ocr</strong></a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[103,104]},"content":"<a href=\"ComputerVision/OCR/spatial_transformer_networks.md\">Spatial Transformer Networks (NIPS 2015)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[104,105]},"content":"<a href=\"ComputerVision/OCR/detecting_text_in_natural_image_with_connectionist_text_proposal_network.md\">CTPN (ECCV 2016)</a> - Text Detection"},{"type":"list_item","depth":3,"payload":{"lines":[105,106]},"content":"<a href=\"ComputerVision/OCR/crnn_ctc.md\">CRNN+CTC (TPAMI 2017)</a> - Text Recognition"},{"type":"list_item","depth":3,"payload":{"lines":[106,107]},"content":"<a href=\"ComputerVision/OCR/EAST-an_efficient_and_accurate_scene_text_detector.md\">EAST (CVPR 2017)</a> - Text Detection"},{"type":"list_item","depth":3,"payload":{"lines":[107,108]},"content":"<a href=\"ComputerVision/OCR/ASTER.md\">ASTER (TPAMI 2018)</a> - Text Recognition"},{"type":"list_item","depth":3,"payload":{"lines":[108,109]},"content":"<a href=\"ComputerVision/OCR/fots_fast_oriented_text_spotting_with_a_unified_network.md\">FOTS (CVPR 2018) üî•</a> - <strong>End2End</strong> Linux Only"},{"type":"list_item","depth":3,"payload":{"lines":[109,110]},"content":"<a href=\"ComputerVision/OCR/towards_end-to-end_text_spotting_in_natural_scenes.md\">2D atttention (ICCV 2019)</a> - <strong>End2End</strong>"},{"type":"list_item","depth":3,"payload":{"lines":[110,111]},"content":"<a href=\"\">Character Region Awareness for Text Detection (CVPR 2019)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[111,112]},"content":"<a href=\"ComputerVision/OCR/convolutional_character_networks.md\">Convolutional Character Networks (ICCV 2019)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[112,113]},"content":"<a href=\"ComputerVision/OCR/aggregation_cross-entropy_for_sequence_recognition.md\">Aggregation Cross-Entropy (CVPR 2019)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[113,114]},"content":"<a href=\"ComputerVision/OCR/mask-textspotter.md\">Mask TextSpotter (TPAMI 2019)</a> - <strong>End2End</strong>"},{"type":"list_item","depth":3,"payload":{"lines":[114,115]},"content":"<a href=\"ComputerVision/OCR/end-to-end_scene_text_recognition_via_iterative_image_rectification.md\">ESIR (CVPR 2019)</a> - Text Recognition"},{"type":"list_item","depth":3,"payload":{"lines":[115,116]},"content":"<a href=\"ComputerVision/OCR/on_recognizing_texts_of_arbitrary_shapes_with_2D_self-attention.md\">SATRN (CVPR 2020)</a> - Text Recognition"},{"type":"list_item","depth":3,"payload":{"lines":[116,117]},"content":"<a href=\"ComputerVision/OCR/decoupled_attention_network_for_text_recognition.md\">DAN (AAAI 2020)</a> - Text Recognition"},{"type":"list_item","depth":3,"payload":{"lines":[117,118]},"content":"<a href=\"ComputerVision/OCR/why_you_should_try_the_real_data_for_the_scene_text_recognition.md\">Yet Another Text Recognizer (ArXiv 2021)</a> - Text Recognition"},{"type":"list_item","depth":3,"payload":{"lines":[118,119]},"content":"<a href=\"ComputerVision/OCR/ABCNet_real-time_scene_text_spotting_with_adaptive_bezier-curve_network.md\">ABCNet (CVPR 2020)</a> - <strong>End2End</strong> Linux Only"},{"type":"list_item","depth":3,"payload":{"lines":[119,120]},"content":"<a href=\"ComputerVision/OCR/abcnet_v2.md\">ABCNet v2 (CVPR 2021) üî•</a> - <strong>End2End</strong> Linux Only"},{"type":"list_item","depth":3,"payload":{"lines":[120,121]},"content":"<a href=\"ComputerVision/OCR/pgnet.md\">PGNet (AAAI 2021) üî•</a> - <strong>End2End</strong>"}]},{"type":"heading","depth":2,"payload":{"lines":[123,124]},"content":"<strong>video analysis</strong>","children":[{"type":"bullet_list","depth":3,"payload":{"lines":[124,137]},"content":"","children":[{"type":"list_item","depth":4,"payload":{"lines":[124,125]},"content":"<a href=\"ComputerVision/VideoAnalysis/video_description.md\">Video Description, Video Reasoning</a>","children":[{"type":"list_item","depth":5,"payload":{"lines":[125,126]},"content":"<a href=\"ComputerVision/VideoAnalysis/describing_videos_by_exploiting_temporal_structure.md\">Describing Videos by Exploiting Temporal Structure (ICCV 2015)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[126,127]},"content":"<a href=\"\">Localizing Moments in Video with Natural Language (ICCV 2017)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[127,128]},"content":"<a href=\"ComputerVision/VideoAnalysis/grounded_video_description.md\"><span>Grounded Video Description (CVPR 2019) üî•</span></a>"},{"type":"list_item","depth":5,"payload":{"lines":[128,129]},"content":"<a href=\"video_relationship_reasoning_using_gated_spatio-temporal_energy_graph.md\">Video Relationship Reasoning Using Gated Spatio-Temporal Energy Graph (GSTEG, CVPR 2019)</a> - CRF"},{"type":"list_item","depth":5,"payload":{"lines":[129,130]},"content":"<a href=\"ComputerVision/VideoAnalysis/adversarial_inference_for_multi-sentence_video_description.md\">Adversarial Inference for Multi-Sentence Video Description (CVPR 2019)</a> - adversarial inference"},{"type":"list_item","depth":5,"payload":{"lines":[130,131]},"content":"<a href=\"ComputerVision/VideoAnalysis/delving_deeper_into_the_decoder_for_video_captioning.md\">Delving Deeper into the Decoder for Video Captioning (ECAI 2020)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[131,132]},"content":"<a href=\"ComputerVision/VideoAnalysis/identity-aware_multi-sentence_video_description.md\">Identity-Aware Multi-Sentence Video Description (ECCV 2020)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[132,133]},"content":"<a href=\"ComputerVision/VideoAnalysis/describing_unseen_videos_via_multi_modal_cooperative_dialog_agents.md\">Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents (ECCV 2020)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[133,134]},"content":"<a href=\"ComputerVision/VideoAnalysis/exploiting_visual_semantic_reasoning_for_video-text_retrieval.md\"><span>Exploiting Visual Semantic Reasoning for Video-Text Retrieval (IJCAL 2020) üî•</span></a>"},{"type":"list_item","depth":5,"payload":{"lines":[134,135]},"content":"<a href=\"ComputerVision/VideoAnalysis/fine-grained_video-text_retrieval_with_hierarchical_graph_reasoning.md\">Fine-Grained Video-Text Retrieval With Hierarchical Graph Reasoning (CVPR 2020)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[135,136]},"content":"<a href=\"ComputerVision/VideoAnalysis/a_hierarchical_reasoning_graph_neural_network_for_the_automatic_scoring_of_answer_transcriptions_in_video_job_interviews.md\">A Hierarchical Reasoning Graph Neural Network for The Automatic Scoring of Answer Transcriptions in Video Job Interviews (ArXiv 2020)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[136,137]},"content":"<a href=\"ComputerVision/VideoAnalysis/reasoning_with_heterogeneous_graph_alignment_for_video_question_answering.md\">Reasoning with Heterogeneous Graph Alignment for Video Question Answering (AAAI 2020)</a>"}]}]},{"type":"bullet_list","depth":3,"payload":{"lines":[139,164]},"content":"","children":[{"type":"list_item","depth":4,"payload":{"lines":[139,140]},"content":"3D Reconstruction","children":[{"type":"list_item","depth":5,"payload":{"lines":[140,141]},"content":"<a href=\"https://www.matthewtancik.com/nerf\"><span>NeRF (ECCV 2020) üî•</span></a>"}]},{"type":"list_item","depth":4,"payload":{"lines":[141,142]},"content":"<a href=\"ComputerVision/VideoAnalysis/temporal_action_detection.md\">Temporal Action Detection</a>","children":[{"type":"list_item","depth":5,"payload":{"lines":[142,143]},"content":"frame-based","children":[{"type":"list_item","depth":6,"payload":{"lines":[143,144]},"content":"<a href=\"\">CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos (CVPR 2017)</a>"},{"type":"list_item","depth":6,"payload":{"lines":[144,145]},"content":"<a href=\"\">TAG: Temporal Action Detection with Structured Segment Networks (ICCV 2017)</a>"},{"type":"list_item","depth":6,"payload":{"lines":[145,146]},"content":"<a href=\"2017https://zhuanlan.zhihu.com/p/84956905\">I3D (CVPR)</a>"},{"type":"list_item","depth":6,"payload":{"lines":[146,147]},"content":"<a href=\"https://arxiv.org/pdf/1812.03982.pdf\">SlowFastNet (ICCV 2019)</a>"}]},{"type":"list_item","depth":5,"payload":{"lines":[147,148]},"content":"proposal-based","children":[{"type":"list_item","depth":6,"payload":{"lines":[148,149]},"content":"<a href=\"ComputerVision/VideoAnalysis/temporal_action_localization_in_untrimmed_videos_via_multi-stage_cnns.md\">SCNN Segment-CNN (CVPR 2016)</a>"},{"type":"list_item","depth":6,"payload":{"lines":[149,150]},"content":"<a href=\"ComputerVision/VideoAnalysis/TURN-TAP.md\">TURN TAP (ICCV 2017)</a>"},{"type":"list_item","depth":6,"payload":{"lines":[150,151]},"content":"<a href=\"ComputerVision/VideoAnalysis/CBR_cascaded_boundary_regression.md\">CBR: Cascaded Boundary Regression (BMVC 2017)</a>"}]},{"type":"list_item","depth":5,"payload":{"lines":[151,152]},"content":"attention-based","children":[{"type":"list_item","depth":6,"payload":{"lines":[152,153]},"content":"<a href=\"https://zhuanlan.zhihu.com/p/357848386\">TimeSformer</a>"}]},{"type":"list_item","depth":5,"payload":{"lines":[153,154]},"content":"self-supervised","children":[{"type":"list_item","depth":6,"payload":{"lines":[154,155]},"content":"<a href=\"https://zhuanlan.zhihu.com/p/250477141\">Visual&amp;CBT</a>"}]},{"type":"list_item","depth":5,"payload":{"lines":[155,156]},"content":"online action detection (ÂºÇÂ∏∏‰∫ã‰ª∂ÁõëÊµã)","children":[{"type":"list_item","depth":6,"payload":{"lines":[156,157]},"content":"<a href=\"ComputerVision/VideoAnalysis/online_action_detection.md\">Online Action Detection (ECCV 2016)</a>"},{"type":"list_item","depth":6,"payload":{"lines":[157,158]},"content":"<a href=\"ComputerVision/VideoAnalysis/RED.md\">RED: Reinforced Encoder-Decoder Networksfor Action Anticipation (BMVC 2017)</a>"},{"type":"list_item","depth":6,"payload":{"lines":[158,159]},"content":"<a href=\"\">Online Action Detection in Untrimmed, Streaming Videos (ECCV 2018)</a>"},{"type":"list_item","depth":6,"payload":{"lines":[159,160]},"content":"<a href=\"ComputerVision/VideoAnalysis/temporal_recurrent_networks_for_online_action_detection.md\">Temporal Recurrent Networks for Online Action Detection (ICCV 2019)</a>"}]},{"type":"list_item","depth":5,"payload":{"lines":[161,162]},"content":"multiple actors","children":[{"type":"list_item","depth":6,"payload":{"lines":[162,163]},"content":"<a href=\"https://drive.google.com/file/d/1ta4UmPSjyjzC7mJCp2AldvMs-uh42NMU/view?usp=drivesdk\">Action Understandingwith Multiple Classes of Actors</a>"},{"type":"list_item","depth":6,"payload":{"lines":[163,164]},"content":"<a href=\"https://www.crcv.ucf.edu/wp-content/uploads/2020/12/Projects_Single-shot-actor-action-detection-in-videos.pdf\">SSA2D</a>"}]}]}]}]}]},{"type":"heading","depth":1,"payload":{"lines":[165,166]},"content":"<a href=\"NaturalLanguageProcessing/nature_language_processing.md\">Natural Language Processing</a>","children":[{"type":"list_item","depth":2,"payload":{"lines":[166,167]},"content":"traditional methods","children":[{"type":"list_item","depth":3,"payload":{"lines":[167,168]},"content":"<a href=\"NaturalLanguageProcessing/word2vec.md\">word2vec (‚ÄéGoogle 2013)</a>"}]},{"type":"list_item","depth":2,"payload":{"lines":[168,169]},"content":"general","children":[{"type":"list_item","depth":3,"payload":{"lines":[169,170]},"content":"<a href=\"NaturalLanguageProcessing/gpt-2.md\">GPT-2/RWC (2019)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[170,171]},"content":"<a href=\"NaturalLanguageProcessing/gpt-3.md\">GPT-3 (NIPS 2020)</a>"}]},{"type":"list_item","depth":2,"payload":{"lines":[171,172]},"content":"full sentence translation","children":[{"type":"list_item","depth":3,"payload":{"lines":[172,173]},"content":"seq2seq"},{"type":"list_item","depth":3,"payload":{"lines":[173,174]},"content":"<a href=\"NaturalLanguageProcessing/a_neural_probabilistic_language_model.md\">A Neural Probabilistic Language Model (JMRL 2003)</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[174,175]},"content":"word embeddings"}]},{"type":"list_item","depth":3,"payload":{"lines":[175,176]},"content":"<a href=\"NaturalLanguageProcessing/self_attentive_sentence_embedding.md\">A Structured Self-Attentive Sentence Embedding (ICLR 2017)</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[176,177]},"content":"Self-Attention"}]},{"type":"list_item","depth":3,"payload":{"lines":[177,178]},"content":"<a href=\"NaturalLanguageProcessing/transformer.md\">Transformer (NIPS 2017)</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[178,179]},"content":"Multi-Head Self-Attention"},{"type":"list_item","depth":4,"payload":{"lines":[179,180]},"content":"Positional Encoding"},{"type":"list_item","depth":4,"payload":{"lines":[180,181]},"content":"Scaled Dot-Product Attention"}]},{"type":"list_item","depth":3,"payload":{"lines":[181,182]},"content":"<a href=\"https://arxiv.org/abs/1807.03819\">Universal transformer (ICLR 2019)</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[182,183]},"content":"Transition Function"},{"type":"list_item","depth":4,"payload":{"lines":[183,184]},"content":"Recurrent Mechanism"}]},{"type":"list_item","depth":3,"payload":{"lines":[184,185]},"content":"<a href=\"NaturalLanguageProcessing/bert.md\"><span>BERT (Arxiv 2018) üëà</span></a>"}]},{"type":"list_item","depth":2,"payload":{"lines":[185,186]},"content":"Simultaneous Translation","children":[{"type":"list_item","depth":3,"payload":{"lines":[186,187]},"content":"<a href=\"https://drive.google.com/file/d/14_1-FOfAf-HZv-y1AHwKpGPpytfZlFcR/view?usp=drivesdk\"><span>STACL (ACL 2019) üëà</span></a>"}]},{"type":"list_item","depth":2,"payload":{"lines":[187,188]},"content":"Machine Translation","children":[{"type":"list_item","depth":3,"payload":{"lines":[188,189]},"content":"<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0885230806000325\">Continuous space language models (CSL 2007)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[189,190]},"content":"<a href=\"https://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf\">Statistical Language Models Based on Neural Networks 2012</a>"}]},{"type":"list_item","depth":2,"payload":{"lines":[191,192]},"content":"Speech Recognition","children":[{"type":"list_item","depth":3,"payload":{"lines":[192,193]},"content":"<a href=\"NaturalLanguageProcessing/whisper.md\">Whisper 2022</a>"},{"type":"list_item","depth":3,"payload":{"lines":[193,194]},"content":""}]},{"type":"list_item","depth":2,"payload":{"lines":[194,195]},"content":"Dialogue Generation"}]},{"type":"heading","depth":1,"payload":{"lines":[198,199]},"content":"<a href=\"GraphTheory/graph_theory.md\">Graph Theory</a>","children":[{"type":"list_item","depth":2,"payload":{"lines":[199,200]},"content":"traditional methods","children":[{"type":"list_item","depth":3,"payload":{"lines":[200,201]},"content":"LINE"},{"type":"list_item","depth":3,"payload":{"lines":[201,202]},"content":"TADW"}]},{"type":"list_item","depth":2,"payload":{"lines":[202,203]},"content":"<a href=\"GraphTheory/graph_embedding.md\">graph embedding</a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[203,204]},"content":"<a href=\"GraphTheory/word2vec.md\">word2vec (ArXiv 2013)</a> - SkipGram"},{"type":"list_item","depth":3,"payload":{"lines":[204,205]},"content":"<a href=\"GraphTheory/deepwalk.md\">DeepWalk (KDD 2014)</a> - graph + word2vec"},{"type":"list_item","depth":3,"payload":{"lines":[205,206]},"content":"<a href=\"GraphTheory/node2vec.md\">node2vec (KDD 2016)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[206,207]},"content":"<a href=\"GraphTheory/item2vec.md\">item2vec (MLSP 2016)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[207,208]},"content":"<a href=\"\">Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification (AAAI 2020)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[208,209]},"content":"<a href=\"\">GraphZoom (ICLR 2020)</a>"}]},{"type":"list_item","depth":2,"payload":{"lines":[210,211]},"content":"<a href=\"GraphTheory/graph_neural_network.md\">graph neural networks</a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[211,212]},"content":"convolutional network","children":[{"type":"list_item","depth":4,"payload":{"lines":[212,213]},"content":"spectral","children":[{"type":"list_item","depth":5,"payload":{"lines":[213,214]},"content":"<a href=\"GraphTheory/spectral_networks_and_deep_locally_connected_networks_on_graphs.md\">Spectral networks and locally connected networks on graphs (ICLR 2014, ÊúÄÊó©ÁöÑÈ¢ëË∞±ÂõæÁ•ûÁªèÁΩëÁªú)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[214,215]},"content":"<a href=\"GraphTheory/deep_convolutional_networks_on_graph-structured_data.md\">Deep convolutional networks on graph-structured data (CoRR 2015)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[215,216]},"content":"<a href=\"GraphTheory/convolutional_neural_networks_on_graphs_with_fast_localized_spectral_filtering.md\">Convolutional Neural Networks on Graphswith Fast Localized Spectral Filtering (Chebyshev expansion, NIPS 2016)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[216,217]},"content":"<a href=\"GraphTheory/semi-supervised_classification_with_graph_convolutional_networks.md\"><span>GCN (ICLR 2017) üî•</span></a>"},{"type":"list_item","depth":5,"payload":{"lines":[217,218]},"content":"AGCN"},{"type":"list_item","depth":5,"payload":{"lines":[218,219]},"content":"GGP"}]},{"type":"list_item","depth":4,"payload":{"lines":[219,220]},"content":"spatial","children":[{"type":"list_item","depth":5,"payload":{"lines":[220,221]},"content":"<a href=\"GraphTheory/convolutional_networks_on_graphs_for_learning_molecular_fingerprints.md\">Convolutional networks on graphs for learning molecular fingerprints (NIPS 2015)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[221,222]},"content":"<a href=\"GraphTheory/diffusion-convolutional_neural_networks.md\">Diffusion-convolutional neural networks (IANIPS 2016)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[222,223]},"content":"Neural FPs"},{"type":"list_item","depth":5,"payload":{"lines":[223,224]},"content":"PATCHY-SAN"},{"type":"list_item","depth":5,"payload":{"lines":[224,225]},"content":"MoNet 2017"},{"type":"list_item","depth":5,"payload":{"lines":[225,226]},"content":"<a href=\"GraphTheory/GraphSAGE.md\">GraphSAGE (NIPS 2017)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[226,227]},"content":"SACNN 2018"},{"type":"list_item","depth":5,"payload":{"lines":[227,228]},"content":"DCNN"}]}]},{"type":"list_item","depth":3,"payload":{"lines":[228,229]},"content":"attention-based network","children":[{"type":"list_item","depth":4,"payload":{"lines":[229,230]},"content":"<a href=\"\">NLNN</a>"},{"type":"list_item","depth":4,"payload":{"lines":[230,231]},"content":"<a href=\"GraphTheory/one-shot_imitation_learning.md\">One-Shot Imitation Learning (neighborhood attention, NIPS 2017)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[231,232]},"content":"<a href=\"GraphTheory/graph_attention_networks.md\"><span>GAT (self-attention, ICLR 2018) üî•</span></a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[233,234]},"content":"spatial-temporal graph"},{"type":"list_item","depth":3,"payload":{"lines":[234,235]},"content":"hierarchical graph","children":[{"type":"list_item","depth":4,"payload":{"lines":[235,236]},"content":"<a href=\"GraphTheory/pairnorm.md\">PairNorm (ICLR 2020)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[236,237]},"content":"<a href=\"GraphTheory/subgraph_neural_networks.md\">Subgraph Neural Networks (NIPS 2020)</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[238,239]},"content":"relational reasoning","children":[{"type":"list_item","depth":4,"payload":{"lines":[239,240]},"content":"<a href=\"GraphTheory/a_simple_neural_network_module_for_relational_reasoning.md\">A simple neural network module for relational reasoning (NIPS 2017)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[240,241]},"content":"<a href=\"GraphTheory/modeling_relational_data_with_graph_convolutional_networks.md\">Relational-GCN (ESWC 2018)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[241,242]},"content":"<a href=\"GraphTheory/VAIN-attentional_multi-agent_predictive_modeling.md\">VAIN (NIPS 2017)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[242,243]},"content":"<a href=\"GraphTheory/composition-based_multi-relational_graph_convolutional_networks.md\">CompGCN (ICLR 2020)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[243,244]},"content":"<a href=\"GraphTheory/temporal_knowledge_graph_reasoning_based_on_evolutional_representation_learning.md\">Temporal Knowledge Graph Reasoning Based on Evolutional Representation Learning (ArXiv 2021)</a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[245,246]},"content":"image segmentation","children":[{"type":"list_item","depth":4,"payload":{"lines":[246,247]},"content":"<a href=\"GraphTheory/zero-shot_video_object_segmentation_via_attentive_graph_neural_networks.md\">AGNN (ICCV 2019)</a> <a href=\"https://github.com/carrierlxk/AGNN\"><img src=\"contents/GitHub.png\" alt=\"\"></a>"}]},{"type":"list_item","depth":3,"payload":{"lines":[248,249]},"content":"<a href=\"GraphTheory/heterogeneous_graph_embedding.md\">Heterogeneity ÂºÇË¥®ÊÄß</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[249,250]},"content":"<a href=\"GraphTheory/metapath2vec.md\">metapath2vec (KDD 2017)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[250,251]},"content":"<a href=\"hindroid-an-intelligent_android_malware_detection_system_based_on_structured.md\">Hindroid (KDD 2017)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[251,252]},"content":"<a href=\"GraphTheory/metagraph2vec.md\">metagraph2vec (PAKDD 2018)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[252,253]},"content":"<a href=\"GraphTheory/heterogeneous_graph_neural_network.md\">Heterogeneous graph neural network (KDD 2919)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[253,254]},"content":"<a href=\"GraphTheory/heterogeneous_graph_attention_network.md\">Heterogeneous graph attention network (HAN, WWW 2019)</a>"},{"type":"list_item","depth":4,"payload":{"lines":[254,255]},"content":"<a href=\"GraphTheory/gatne.md\">GATNE (KDD 2019)</a>"}]}]}]},{"type":"heading","depth":1,"payload":{"lines":[257,258]},"content":"<a href=\"OutlierDetection/outlier_detection.md\">Outlier Detection</a>","children":[{"type":"list_item","depth":2,"payload":{"lines":[258,259]},"content":"One class classification / Out-of-Distribution"},{"type":"list_item","depth":2,"payload":{"lines":[259,260]},"content":"Open set/world recognition","children":[{"type":"list_item","depth":3,"payload":{"lines":[260,261]},"content":"<a href=\"OutlierDetection/towards_open_world_object_detAection.md\">Towards Open World Object Detection (CVPR 2021)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[261,262]},"content":"<a href=\"OutlierDetection/ow-detr.md\">OW-DETR (arxiv 2021)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[262,263]},"content":"<a href=\"OutlierDetection/open-world_object_detection_and_tracking.md\">Open-world Object Detection and Tracking (cmu 2021)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[263,264]},"content":"<a href=\"OutlierDetection/re-owod.md\">Revisiting Open World Object Detection (arXiv 2022)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[264,265]},"content":"<a href=\"\">VOS (ICLR 2022): Learning What You Don‚Äôt Know By Virtual Outlier Synthesis</a>"}]}]},{"type":"heading","depth":1,"payload":{"lines":[267,268]},"content":"<a href=\"PointCloud/point_cloud.md\">Point Cloud</a>","children":[{"type":"list_item","depth":2,"payload":{"lines":[268,269]},"content":"Detection","children":[{"type":"list_item","depth":3,"payload":{"lines":[269,270]},"content":"<a href=\"PointCloud/point_net.md\">PointNet 2017</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[270,271]},"content":"classification (feature extraction+max pooling) + segmentation network (global+point feature concatenation)"}]},{"type":"list_item","depth":3,"payload":{"lines":[271,272]},"content":"<a href=\"\">PointNet++ 2017</a>"},{"type":"list_item","depth":3,"payload":{"lines":[272,273]},"content":"<a href=\"PointCloud/voxelnet.md\">VoxelNet 2018</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[273,274]},"content":"consider voxel in information aggregation -&gt; only process voxel feature"}]},{"type":"list_item","depth":3,"payload":{"lines":[274,275]},"content":"<a href=\"\">PointPillars 2019</a>"},{"type":"list_item","depth":3,"payload":{"lines":[275,276]},"content":"<a href=\"PointCloud/mlcvnet.md\">Multi-Level Context VoteNet 2020</a>"},{"type":"list_item","depth":3,"payload":{"lines":[276,277]},"content":"<a href=\"\">PV-RCNN++ 2022</a>"},{"type":"list_item","depth":3,"payload":{"lines":[277,278]},"content":"<a href=\"PointCloud/BADet.md\">BADet 2022</a>"}]},{"type":"list_item","depth":2,"payload":{"lines":[279,280]},"content":"Segmentation","children":[{"type":"list_item","depth":3,"payload":{"lines":[280,281]},"content":"<a href=\"https://arxiv.org/abs/1710.07368\">SqueezeSeg 2017</a>"},{"type":"list_item","depth":3,"payload":{"lines":[281,282]},"content":"<a href=\"https://arxiv.org/abs/1807.06288\">PointSeg 2018</a>"},{"type":"list_item","depth":3,"payload":{"lines":[282,283]},"content":"<a href=\"https://arxiv.org/pdf/2008.01550.pdf\">Cylinder3D 2020</a>"}]},{"type":"list_item","depth":2,"payload":{"lines":[284,285]},"content":"Sensor Fusion","children":[{"type":"list_item","depth":3,"payload":{"lines":[285,286]},"content":"<a href=\"PointCloud/second.md\">SECOND 2018</a>"},{"type":"list_item","depth":3,"payload":{"lines":[286,287]},"content":"<a href=\"PointCloud/frustum_pointnets.md\">Frustum PointNets 2018</a>"},{"type":"list_item","depth":3,"payload":{"lines":[287,288]},"content":"<a href=\"PointCloud/pointfusion.md\">PointFusion 2018</a>"},{"type":"list_item","depth":3,"payload":{"lines":[288,289]},"content":"<a href=\"PointCloud/CLOCs.md\">CLOCs 2020: Camera-LiDAR Object Candidates Fusion for 3D Object Detection</a>"},{"type":"list_item","depth":3,"payload":{"lines":[289,290]},"content":"<a href=\"PointCloud/frustum_pointpillars.md\">Frustum-PointPillars 2021</a>","children":[{"type":"list_item","depth":4,"payload":{"lines":[290,291]},"content":"frustum probability mask (from image) + pointnet (bird-view 2D grid)"}]},{"type":"list_item","depth":3,"payload":{"lines":[291,292]},"content":"<a href=\"PointCloud/object_detection_in_3D_point_clouds_via_local_correlation-aware_point_embedding.md\">Object Detection in 3D Point Clouds via Local Correlation-Aware Point Embedding 2021</a>"},{"type":"list_item","depth":3,"payload":{"lines":[292,293]},"content":"<a href=\"PointCloud/autoalign.md\">AutoAlign 2022</a>: attention + Cross-modal Feature Interaction"}]},{"type":"list_item","depth":2,"payload":{"lines":[294,295]},"content":"2D Image -&gt; 3D Object Detection","children":[{"type":"list_item","depth":3,"payload":{"lines":[295,296]},"content":"<a href=\"PointCloud/imvoxelnet.md\">ImVoxelNet (WACV 2022)</a>"}]}]},{"type":"heading","depth":1,"payload":{"lines":[298,299]},"content":"<a href=\"Recommendation/recommendation.md\">Recommendation</a>","children":[{"type":"list_item","depth":2,"payload":{"lines":[299,300]},"content":"<a href=\"Recommendation/lira.md\">LIRA 1998</a>"},{"type":"list_item","depth":2,"payload":{"lines":[300,301]},"content":"<a href=\"https://drive.google.com/file/d/16bSJlpzmGmxAgh-U_zMyl1QJvqhcY_vO/view?usp=drivesdk\">SimRank 2002</a>"},{"type":"list_item","depth":2,"payload":{"lines":[301,302]},"content":"<a href=\"https://drive.google.com/file/d/1zPbx8cq_pOqljk3xCmJ0HsafqK7dFP6H/view?usp=drivesdk\">Matrix Factorization 2009</a>"},{"type":"list_item","depth":2,"payload":{"lines":[302,303]},"content":"<a href=\"https://drive.google.com/file/d/1LB-iSCGi0UDCZBsDgZpFA-4P9U8NJ8ro/view?usp=drivesdk\">WSABIE rank loss 2011</a>"},{"type":"list_item","depth":2,"payload":{"lines":[303,304]},"content":"<a href=\"https://drive.google.com/file/d/1FTxR2qfzXrP2dui53DGM19_fvEUh9BVj/view?usp=drivesdk\">XGBoost 2016</a>"},{"type":"list_item","depth":2,"payload":{"lines":[304,305]},"content":"<a href=\"Recommendation/deep_neural_networks_for_youtube_ecommendations.md\">Deep Neural Networks for YouTube Recommendations 2016</a>"},{"type":"list_item","depth":2,"payload":{"lines":[305,306]},"content":"<a href=\"Recommendation/dlrm.md\">DLRM 2019</a>"},{"type":"list_item","depth":2,"payload":{"lines":[306,307]},"content":"<a href=\"https://drive.google.com/file/d/1yLhp2yUVHHmbqtWOxVdLUGoTSW4Odjrm/view?usp=drivesdk\">Two-tower model 2020</a>"}]},{"type":"heading","depth":1,"payload":{"lines":[310,311]},"content":"<a href=\"UnsupervisedLearning/unsupervised_learning.md\">Unsupervised Learning</a>","children":[{"type":"table","depth":2,"payload":{"lines":[312,316]},"content":"","children":[{"type":"thead","depth":3,"payload":{"lines":[312,313]},"content":"","children":[{"type":"th","depth":4,"payload":{"lines":[312,313]},"content":""},{"type":"th","depth":4,"payload":{"lines":[312,313]},"content":"With Teacher"},{"type":"th","depth":4,"payload":{"lines":[312,313]},"content":"Without Teacher"}]},{"type":"tbody","depth":3,"payload":{"lines":[314,316]},"content":"","children":[{"type":"tr","depth":4,"payload":{},"content":"","children":[{"type":"td","depth":5,"payload":{},"content":"<strong>Active</strong>"},{"type":"td","depth":5,"payload":{},"content":"Reinforcement Learning/Active Learning"},{"type":"td","depth":5,"payload":{},"content":"Intrinsic Motivation/Exploration"}]},{"type":"tr","depth":4,"payload":{},"content":"","children":[{"type":"td","depth":5,"payload":{},"content":"<strong>Passive</strong>"},{"type":"td","depth":5,"payload":{},"content":"Supervised Learning"},{"type":"td","depth":5,"payload":{},"content":"Unsupervised Learning"}]}]}]},{"type":"heading","depth":2,"payload":{"lines":[317,318]},"content":"<a href=\"UnsupervisedLearning/active_learning.md\">active learning</a>"},{"type":"heading","depth":2,"payload":{"lines":[319,320]},"content":"<a href=\"UnsupervisedLearning/representation_learning.md\">representation learning</a>","children":[{"type":"list_item","depth":3,"payload":{"lines":[320,321]},"content":"Colorization 2016"},{"type":"list_item","depth":3,"payload":{"lines":[321,322]},"content":"<a href=\"https://arxiv.org/abs/1703.10593\">Cycle-GAN (ICCV 2017)</a>"},{"type":"list_item","depth":3,"payload":{"lines":[322,323]},"content":"Unsupervised word translation 2018"},{"type":"list_item","depth":3,"payload":{"lines":[323,324]},"content":"Deep clustering 2018"},{"type":"list_item","depth":3,"payload":{"lines":[324,325]},"content":"<a href=\"https://drive.google.com/file/d/1FdfNcJDI0Y4QbVRuW79-c9iSvd8VErGp/view\">XLM-R 2020</a>"},{"type":"list_item","depth":3,"payload":{"lines":[325,326]},"content":"<a href=\"UnsupervisedLearning/tabnet.md\">TabNet 2020</a>"},{"type":"list_item","depth":3,"payload":{"lines":[326,327]},"content":"<a href=\"UnsupervisedLearning/on_mutual_infomration_maximization_for_representation_learning.md\">On mutual infomration maximization for representation learning 2020</a>"}]},{"type":"heading","depth":2,"payload":{"lines":[329,330]},"content":"autoregressive networks","children":[{"type":"list_item","depth":3,"payload":{"lines":[330,331]},"content":"WaveNets 2016"},{"type":"list_item","depth":3,"payload":{"lines":[331,332]},"content":"PixelRNN 2016"}]}]},{"type":"heading","depth":1,"payload":{"lines":[335,336]},"content":"Transfer Learning","children":[{"type":"list_item","depth":2,"payload":{"lines":[336,337]},"content":"<a href=\"\">Audio Spoofing Verification using Deep Convolutional Neural Networks by Transfer Learning (CoRR 2020)</a>"}]},{"type":"heading","depth":1,"payload":{"lines":[339,340]},"content":"<a href=\"DomainAdaption\\domain_adaption.md\">Domain Adaptation</a>"},{"type":"heading","depth":1,"payload":{"lines":[342,343]},"content":"Meta-Learning"},{"type":"heading","depth":1,"payload":{"lines":[345,346]},"content":"Continual Learning"},{"type":"heading","depth":1,"payload":{"lines":[349,350]},"content":"Data Augmentation"},{"type":"heading","depth":1,"payload":{"lines":[353,354]},"content":"Concepts","children":[{"type":"list_item","depth":2,"payload":{"lines":[354,355]},"content":"<a href=\"NaturalLanguageProcessing/gpt-3.md\">data contamination</a>: training dataset can potentially include content from test datasets"},{"type":"list_item","depth":2,"payload":{"lines":[355,356]},"content":"<a href=\"NaturalLanguageProcessing/gpt-2.md\">sentiment analysis</a>: ÊÉÖÁª™ÂàÜÊûê"}]}]},{})</script>
</body>
</html>
