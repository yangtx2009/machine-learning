<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.1.5/dist/style.min.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@6.6.0"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.2.3"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.1.5/dist/index.umd.min.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, data) => {
        const {
          Markmap
        } = getMarkmap();
        window.mm = Markmap.create('svg#mindmap', getOptions == null ? void 0 : getOptions(), data);
      })(() => window.markmap,null,{"t":"heading","d":1,"p":{"lines":[0,1]},"v":"Machine Learning","c":[{"t":"heading","d":2,"p":{"lines":[1,2]},"v":"General","c":[{"t":"list_item","d":4,"p":{"lines":[2,3]},"v":"<a href=\"https://arxiv.org/abs/1512.03385\">Resnet 2015</a>","c":[{"t":"list_item","d":6,"p":{"lines":[3,4]},"v":"residual learning"}]},{"t":"list_item","d":4,"p":{"lines":[4,5]},"v":"<a href=\"https://arxiv.org/abs/1607.06450\">Layer normalization 2016</a>","c":[{"t":"list_item","d":6,"p":{"lines":[5,6]},"v":"layer normalization"}]},{"t":"list_item","d":4,"p":{"lines":[6,7]},"v":"<a href=\"https://drive.google.com/file/d/1QlGDm0RTg7SJFegwRIll3Egnvczp-mWo/view?usp=drivesdk\">ResNeSt 2020</a>(split-attention)"}]},{"t":"heading","d":2,"p":{"lines":[9,10]},"v":"Action Predition"},{"t":"heading","d":2,"p":{"lines":[13,14]},"v":"<a href=\"contents/ML/ComputerVision/computer_vision.md\">Computer Vision</a>","c":[{"t":"heading","d":3,"p":{"lines":[14,15]},"v":"detection","c":[{"t":"list_item","d":5,"p":{"lines":[15,16]},"v":"R-CNN 2014"},{"t":"list_item","d":5,"p":{"lines":[16,17]},"v":"Fast R-CNN 2015"},{"t":"list_item","d":5,"p":{"lines":[17,18]},"v":"<span>Faster R-CNN 2015 ðŸ”¥</span>"},{"t":"list_item","d":5,"p":{"lines":[18,19]},"v":"SSD 2016"},{"t":"list_item","d":5,"p":{"lines":[19,20]},"v":"<a href=\"contents/ML/ComputerVision/Detection/yolo.md\">YOLO (CVPR 2016)</a>"},{"t":"list_item","d":5,"p":{"lines":[20,21]},"v":"<a href=\"contents/ML/ComputerVision/Detection/yolo.md\">YOLOv2 (CVPR 2017)</a>"},{"t":"list_item","d":5,"p":{"lines":[21,22]},"v":"RetinaNet 2017"},{"t":"list_item","d":5,"p":{"lines":[22,23]},"v":"<a href=\"contents/ML/ComputerVision/Detection/yolo.md\"><span>YOLOv3 2019 ðŸ”¥</span></a>"},{"t":"list_item","d":5,"p":{"lines":[23,24]},"v":"<a href=\"contents/ML/ComputerVision/Detection/yolo.md\">YOLOv4 2020</a>"},{"t":"list_item","d":5,"p":{"lines":[24,25]},"v":"RelationNet++ 2020"},{"t":"list_item","d":5,"p":{"lines":[25,26]},"v":"DETR 2020"},{"t":"list_item","d":5,"p":{"lines":[26,27]},"v":"UP-DETR 2020"}]},{"t":"heading","d":3,"p":{"lines":[28,29]},"v":"segmentation","c":[{"t":"list_item","d":5,"p":{"lines":[29,30]},"v":"Regional proposal based","c":[{"t":"list_item","d":7,"p":{"lines":[30,31]},"v":"<a href=\"contents/ML/ComputerVision/Segmentation/mask_rcnn.md\"><span>Mask R-CNN (ICCV 2017) ðŸ”¥</span></a>","c":[{"t":"list_item","d":9,"p":{"lines":[31,32]},"v":"Binary ROI mask"},{"t":"list_item","d":9,"p":{"lines":[32,33]},"v":"RoIAlign"}]},{"t":"list_item","d":7,"p":{"lines":[33,34]},"v":"<a href=\"https://arxiv.org/abs/1802.05591\">LaneNet (IEEE IV 2018)</a>"},{"t":"list_item","d":7,"p":{"lines":[34,35]},"v":"<a href=\"https://ieeexplore.ieee.org/document/8953609\">Mask Scoring R-CNN (CVPR 2019)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[36,37]},"v":"RNN based","c":[{"t":"list_item","d":7,"p":{"lines":[37,38]},"v":"ReSeg"},{"t":"list_item","d":7,"p":{"lines":[38,39]},"v":"MDRNNs"}]},{"t":"list_item","d":5,"p":{"lines":[40,41]},"v":"Upsampling + Deconvolution","c":[{"t":"list_item","d":7,"p":{"lines":[41,42]},"v":"<a href=\"https://arxiv.org/abs/1605.06211v1\">FCN (CVPR 2015 &amp; TPAMI 2017)</a>"},{"t":"list_item","d":7,"p":{"lines":[42,43]},"v":"SetNet"},{"t":"list_item","d":7,"p":{"lines":[43,44]},"v":"<a href=\"https://drive.google.com/file/d/1GIOJgIe1BzChxoIWJyZq4G7EQOAE4OPY/view?usp=drivesdk\">U-net (MICCAI 2015)</a>"},{"t":"list_item","d":7,"p":{"lines":[44,45]},"v":"<a href=\"https://drive.google.com/file/d/1wIo5dLL_Sn2Bxlo4cGx4YacONw1mJU6N/view?usp=drivesdk\">FastFCN 2019</a>"}]},{"t":"list_item","d":5,"p":{"lines":[46,47]},"v":"CRF/MRF","c":[{"t":"list_item","d":7,"p":{"lines":[47,48]},"v":"<a href=\"https://drive.google.com/file/d/1X0S9WRAzMTG0hQbaysdR60prPdhPRf9E/view?usp=drivesdk\">DeepLab (ICLR 2015 &amp; ICCV 2015)</a>","c":[{"t":"list_item","d":9,"p":{"lines":[48,49]},"v":"Fully connected CRF"},{"t":"list_item","d":9,"p":{"lines":[49,50]},"v":"Atrous (Dilated) Convolution"}]},{"t":"list_item","d":7,"p":{"lines":[50,51]},"v":"<a href=\"https://drive.google.com/file/d/1oQTA8xbPvoBV0IQbOnnBsKLZVocJieQg/view?usp=drivesdk\">CRF Meet Deep Neural Networks for Semantic Segmentation 2018</a>"}]},{"t":"list_item","d":5,"p":{"lines":[52,53]},"v":"Gated-SCNN 2019"},{"t":"list_item","d":5,"p":{"lines":[54,55]},"v":"Interactive","c":[{"t":"list_item","d":7,"p":{"lines":[55,56]},"v":"<a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Interactive_Image_Segmentation_With_First_Click_Attention_CVPR_2020_paper.pdf\">Interactive Image Segmentation With First Click Attention (CVPR 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[56,57]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9156403\">F-BRS (CVPR 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[57,58]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9157733\">Interactive Object Segmentation With Inside-Outside Guidance (CVPR 2020)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[59,60]},"v":"Moving Object","c":[{"t":"list_item","d":7,"p":{"lines":[60,61]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9156405\">An End-to-End Edge Aggregation Network for Moving Object Segmentation (CVPR 2020)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[62,63]},"v":"Transparent Object","c":[{"t":"list_item","d":7,"p":{"lines":[63,64]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9156916\">Deep Polarization Cues for Transparent Object Segmentation (CVPR 2020)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[65,66]},"v":"Referring è½¬ä»‹","c":[{"t":"list_item","d":7,"p":{"lines":[66,67]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9157191\">PhraseCut (CVPR 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[67,68]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9156414\">Referring Image Segmentation via Cross-Modal Progressive Comprehension (CVPR 2020)</a>"}]},{"t":"list_item","d":5,"p":{"lines":[68,69]},"v":"Multi-Object Tracking","c":[{"t":"list_item","d":7,"p":{"lines":[69,70]},"v":"<a href=\"https://ieeexplore.ieee.org/document/9157138\">Learning Multi-Object Tracking and Segmentation From Automatic Annotations (CVPR 2020)</a>"}]}]},{"t":"heading","d":3,"p":{"lines":[71,72]},"v":"lane detection","c":[{"t":"list_item","d":5,"p":{"lines":[72,73]},"v":"Segmentation-based","c":[{"t":"list_item","d":7,"p":{"lines":[73,74]},"v":"<a href=\"contents/ML/ComputerVision/LaneDetection/scnn.md\">SCNN (ISCA 2017)</a>"},{"t":"list_item","d":7,"p":{"lines":[74,75]},"v":"<a href=\"contents/ML/ComputerVision/LaneDetection/enet_sad.md\">ENet-SAD (ICCV 2019)</a>"},{"t":"list_item","d":7,"p":{"lines":[75,76]},"v":"CurveLanes-NAS"},{"t":"list_item","d":7,"p":{"lines":[76,77]},"v":"LaneNet 2019"},{"t":"list_item","d":7,"p":{"lines":[77,78]},"v":"RESA 2020"},{"t":"list_item","d":7,"p":{"lines":[78,79]},"v":"SUPER 2020"},{"t":"list_item","d":7,"p":{"lines":[79,80]},"v":"ERFNet-IntRA-KD 2020"}]},{"t":"list_item","d":5,"p":{"lines":[80,81]},"v":"Row-wise","c":[{"t":"list_item","d":7,"p":{"lines":[81,82]},"v":"E2E-LMD"},{"t":"list_item","d":7,"p":{"lines":[82,83]},"v":"IntRA-KD"}]},{"t":"list_item","d":5,"p":{"lines":[83,84]},"v":"Other approaches"}]},{"t":"heading","d":3,"p":{"lines":[85,86]},"v":"generation","c":[{"t":"list_item","d":5,"p":{"lines":[86,87]},"v":"image generation","c":[{"t":"list_item","d":7,"p":{"lines":[87,88]},"v":"Parzen window-based log-likelihood"},{"t":"list_item","d":7,"p":{"lines":[88,89]},"v":"<a href=\"https://arxiv.org/abs/1406.2661\">GAN 2014</a>"}]},{"t":"list_item","d":5,"p":{"lines":[89,90]},"v":"textual descriptions","c":[{"t":"list_item","d":7,"p":{"lines":[90,91]},"v":"<a href=\"https://arxiv.org/abs/1502.03044\"><span>Neural Image Caption Generation with Visual Attention ðŸ‘ˆ</span></a>"}]}]},{"t":"heading","d":3,"p":{"lines":[92,93]},"v":"ocr"},{"t":"heading","d":3,"p":{"lines":[94,95]},"v":"video analysis","c":[{"t":"list_item","d":5,"p":{"lines":[95,96]},"v":"<a href=\"contents\\ML\\ComputerVision\\VideoAnalysis\\video_description.md\">Video Description</a>","c":[{"t":"list_item","d":7,"p":{"lines":[96,97]},"v":"<a href=\"contents\\ML\\ComputerVision\\VideoAnalysis\\grounded_video_description.md\"><span>Grounded Video Description (CVPR 2019) ðŸ”¥</span></a>"},{"t":"list_item","d":7,"p":{"lines":[97,98]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/adversarial_inference_for_multi-sentence_video_description.md\">Adversarial Inference for Multi-Sentence Video Description (CVPR 2019)</a>","c":[{"t":"list_item","d":9,"p":{"lines":[98,99]},"v":"adversarial techniques for inference"}]},{"t":"list_item","d":7,"p":{"lines":[99,100]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/identity-aware_multi-sentence_video_description.md\">Identity-Aware Multi-SentenceVideo Description (ECCV 2020)</a>"},{"t":"list_item","d":7,"p":{"lines":[100,101]},"v":"<a href=\"contents/ML/ComputerVision/VideoAnalysis/describing_unseen_videos_via_multi_modal_cooperative_dialog_agents.md\">Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents (ECCV 2020)</a>"}]}]}]},{"t":"heading","d":2,"p":{"lines":[103,104]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/nature_language_processing.md\">Natural Language Processing</a>","c":[{"t":"list_item","d":4,"p":{"lines":[104,105]},"v":"traditional methods","c":[{"t":"list_item","d":6,"p":{"lines":[105,106]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/word2vec.md\">word2vec (â€ŽGoogle 2013)</a>"}]},{"t":"list_item","d":4,"p":{"lines":[106,107]},"v":"full sentence translation","c":[{"t":"list_item","d":6,"p":{"lines":[107,108]},"v":"seq2seq"},{"t":"list_item","d":6,"p":{"lines":[108,109]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/a_neural_probabilistic_language_model.md\">A Neural Probabilistic Language Model (JMRL 2003)</a>","c":[{"t":"list_item","d":8,"p":{"lines":[109,110]},"v":"word embeddings"}]},{"t":"list_item","d":6,"p":{"lines":[110,111]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/self_attentive_sentence_embedding.md\">A Structured Self-Attentive Sentence Embedding (ICLR 2017)</a>","c":[{"t":"list_item","d":8,"p":{"lines":[111,112]},"v":"Self-Attention"}]},{"t":"list_item","d":6,"p":{"lines":[112,113]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/transformer.md\">Transformer (NIPS 2017)</a>","c":[{"t":"list_item","d":8,"p":{"lines":[113,114]},"v":"Multi-Head Self-Attention"},{"t":"list_item","d":8,"p":{"lines":[114,115]},"v":"Positional Encoding"},{"t":"list_item","d":8,"p":{"lines":[115,116]},"v":"Scaled Dot-Product Attention"}]},{"t":"list_item","d":6,"p":{"lines":[116,117]},"v":"<a href=\"https://arxiv.org/abs/1807.03819\">Universal transformer (ICLR 2019)</a>","c":[{"t":"list_item","d":8,"p":{"lines":[117,118]},"v":"Transition Function"},{"t":"list_item","d":8,"p":{"lines":[118,119]},"v":"Recurrent Mechanism"}]},{"t":"list_item","d":6,"p":{"lines":[119,120]},"v":"<a href=\"contents/ML/NaturalLanguageProcessing/bert.md\"><span>BERT (Arxiv 2018) ðŸ‘ˆ</span></a>"}]},{"t":"list_item","d":4,"p":{"lines":[120,121]},"v":"Simultaneous Translation","c":[{"t":"list_item","d":6,"p":{"lines":[121,122]},"v":"<a href=\"https://drive.google.com/file/d/14_1-FOfAf-HZv-y1AHwKpGPpytfZlFcR/view?usp=drivesdk\"><span>STACL (ACL 2019) ðŸ‘ˆ</span></a>"}]},{"t":"list_item","d":4,"p":{"lines":[122,123]},"v":"Machine Translation","c":[{"t":"list_item","d":6,"p":{"lines":[123,124]},"v":"<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0885230806000325\">Continuous space language models (CSL 2007)</a>"},{"t":"list_item","d":6,"p":{"lines":[124,125]},"v":"<a href=\"https://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf\">Statistical Language Models Based on Neural Networks 2012</a>"}]}]},{"t":"heading","d":2,"p":{"lines":[127,128]},"v":"Outlier Detection","c":[{"t":"list_item","d":4,"p":{"lines":[128,129]},"v":"one class classification"},{"t":"list_item","d":4,"p":{"lines":[129,130]},"v":"open set recognition"}]},{"t":"heading","d":2,"p":{"lines":[132,133]},"v":"Point Cloud"},{"t":"heading","d":2,"p":{"lines":[135,136]},"v":"<a href=\"contents/ML/Recommendation/recommendation.md\">Recommendation</a>","c":[{"t":"list_item","d":4,"p":{"lines":[136,137]},"v":"<a href=\"contents/ML/Recommendation/lira.md\">LIRA 1998</a>"},{"t":"list_item","d":4,"p":{"lines":[137,138]},"v":"<a href=\"https://drive.google.com/file/d/16bSJlpzmGmxAgh-U_zMyl1QJvqhcY_vO/view?usp=drivesdk\">SimRank 2002</a>"},{"t":"list_item","d":4,"p":{"lines":[138,139]},"v":"<a href=\"https://drive.google.com/file/d/1zPbx8cq_pOqljk3xCmJ0HsafqK7dFP6H/view?usp=drivesdk\">Matrix Factorization 2009</a>"},{"t":"list_item","d":4,"p":{"lines":[139,140]},"v":"<a href=\"https://drive.google.com/file/d/1LB-iSCGi0UDCZBsDgZpFA-4P9U8NJ8ro/view?usp=drivesdk\">WSABIE rank loss 2011</a>"},{"t":"list_item","d":4,"p":{"lines":[140,141]},"v":"<a href=\"https://drive.google.com/file/d/1FTxR2qfzXrP2dui53DGM19_fvEUh9BVj/view?usp=drivesdk\">XGBoost 2016</a>"},{"t":"list_item","d":4,"p":{"lines":[141,142]},"v":"<a href=\"contents/ML/Recommendation/deep_neural_networks_for_youtube_ecommendations.md\">Deep Neural Networks for YouTube Recommendations 2016</a>"},{"t":"list_item","d":4,"p":{"lines":[142,143]},"v":"<a href=\"contents/ML/Recommendation/dlrm.md\">DLRM 2019</a>"},{"t":"list_item","d":4,"p":{"lines":[143,144]},"v":"<a href=\"https://drive.google.com/file/d/1yLhp2yUVHHmbqtWOxVdLUGoTSW4Odjrm/view?usp=drivesdk\">Two-tower model 2020</a>"}]},{"t":"heading","d":2,"p":{"lines":[147,148]},"v":"<a href=\"contents/ML/UnsupervisedLearning/unsupervised_learning.md\">Unsupervised Learning</a>","c":[{"t":"heading","d":3,"p":{"lines":[150,151]},"v":"<a href=\"contents/ML/UnsupervisedLearning/active_learning.md\">active learning</a>"},{"t":"heading","d":3,"p":{"lines":[152,153]},"v":"<a href=\"contents/ML/UnsupervisedLearning/representation_learning.md\">representation learning</a>","c":[{"t":"list_item","d":5,"p":{"lines":[153,154]},"v":"Colorization 2016"},{"t":"list_item","d":5,"p":{"lines":[154,155]},"v":"<a href=\"https://arxiv.org/abs/1703.10593\">Cycle-GAN (ICCV 2017)</a>"},{"t":"list_item","d":5,"p":{"lines":[155,156]},"v":"Unsupervised word translation 2018"},{"t":"list_item","d":5,"p":{"lines":[156,157]},"v":"Deep clustering 2018"},{"t":"list_item","d":5,"p":{"lines":[157,158]},"v":"<a href=\"https://drive.google.com/file/d/1FdfNcJDI0Y4QbVRuW79-c9iSvd8VErGp/view\">XLM-R 2020</a>"},{"t":"list_item","d":5,"p":{"lines":[158,159]},"v":"<a href=\"contents/ML/UnsupervisedLearning/tabnet.md\">TabNet 2020</a>"},{"t":"list_item","d":5,"p":{"lines":[159,160]},"v":"<a href=\"contents/ML/UnsupervisedLearning/on_mutual_infomration_maximization_for_representation_learning.md\">On mutual infomration maximization for representation learning 2020</a>"}]},{"t":"heading","d":3,"p":{"lines":[162,163]},"v":"autoregressive networks","c":[{"t":"list_item","d":5,"p":{"lines":[163,164]},"v":"WaveNets 2016"},{"t":"list_item","d":5,"p":{"lines":[164,165]},"v":"PixelRNN 2016"}]}]}]})</script>
</body>
</html>
