# Representation Learning

理解（每层）神经网络对数据的表达，即描述数据的内部语言

# Topics
## Clustering
  - Deep clustering 2018
  - Colorization 2016
  
## Text
Key ideas of learning representatin from text is by predicting a word from the context (or vice versa) e.g. BERT from NIPS 2020
  - [XLM-R 2020](https://drive.google.com/file/d/1FdfNcJDI0Y4QbVRuW79-c9iSvd8VErGp/view)
    - transfer learning in corss-lingual RL in large scale
  
## Domain Mapping
  - [Cycle-GAN (ICCV 2017)](https://arxiv.org/abs/1703.10593)
  
## Machine Translation
  - [Unsupervised word translation (ICLR 2018)](https://arxiv.org/abs/1710.04087)
    1. Learn embedding separately
    2. Learn joint space via adversarial training + refinement
  
## Reasoning
  - [TabNet 2020](tabnet.md)
    - feature selection methods
  
## Others
  - [On mutual infomration maximization for representation learning (ICLR 2020)](on_mutual_infomration_maximization_for_representation_learning.md)