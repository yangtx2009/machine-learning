# Video Analysis

## Keywords
- multi-modal task
- video captioning



## Research Teams

| Team | Member |
|------|--------|
| Facebook AI | Luowei Zhou, Yannis Kalantidis, Xinlei Chen, **Marcus Rohrbach** |
| University of Michigan             | Luowei Zhou, Jason J.Corso |
| University of California, Berkeley | **Trevor Darrell**, **Anna Rohrbach** |
| University of Washington           | **Jae Sung Park** |
| Texas State University             | Ye Zhu, Yan Yan |
| University of Technology Sydney    | Yu Wu, Yi Yang,   |
| Baidu Research                     | Yu Wu |
| Beijing University of Posts and Telecommunications (北京邮电大学)| Zerun Feng, Zhimin Zeng, Zheng Li
| Beijing Laboratory of Advanced Information Networks 先进信息网络北京实验室 | Zhimin Zeng, Caili Guo |

## Metrics
- [VizSeq (IJCNLP 2019)](vizseq.md) summarized a list of metrics used in video description.

## Dataset
- [ActivityNet Captions](http://activity-net.org/download.html)
- [ActivityNet-Entities](https://github.com/facebookresearch/ActivityNet-Entities)
  - evaluate how grounded or “true” such model are to the video they describe.
- [ActivityNet Captions PapersWithCode](https://paperswithcode.com/dataset/activitynet-captions)
- [Flickr30k Entities dataset](http://bryanplummer.com/Flickr30kEntities/)
- [VQA](https://visualqa.org/)
- [GuessWhat](https://arxiv.org/abs/1611.08481)
- [DAVIS](https://davischallenge.org/)